#!/usr/bin/env python3
"""
çœŸæ­£æ··åˆç²¾åº¦åŠŸèƒ½æµ‹è¯•è„šæœ¬
éªŒè¯å¤šç§é‡åŒ–æ ¼å¼å…±å­˜ï¼Œä¿æŒå‹ç¼©æ ¼å¼ä»¥èŠ‚çœGPUå­˜å‚¨
"""

import os
import sys
import logging
import yaml
import torch
from pathlib import Path

# æ·»åŠ SGLangè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "python"))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def test_compressed_weight_structures():
    """æµ‹è¯•å‹ç¼©æƒé‡æ•°æ®ç»“æ„"""
    print("=" * 60)
    print("æµ‹è¯•å‹ç¼©æƒé‡æ•°æ®ç»“æ„")
    print("=" * 60)
    
    try:
        from sglang.srt.model_loader.true_mixed_precision_loader import (
            CompressedWeight, WeightFormat
        )
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„GPTQå‹ç¼©æƒé‡
        qweight = torch.randint(0, 16, (256, 768), dtype=torch.int32)
        qzeros = torch.randint(0, 16, (16, 96), dtype=torch.int32)
        scales = torch.randn(16, 768, dtype=torch.float16)
        
        metadata = {
            'qweight': qweight,
            'qzeros': qzeros,
            'scales': scales,
            'bits': 4,
            'group_size': 128
        }
        
        compressed_weight = CompressedWeight(
            format=WeightFormat.GPTQ_INT4,
            data=qweight,
            metadata=metadata,
            original_shape=(768, 2048),
            compressed_size=(qweight.numel() + qzeros.numel() + scales.numel()) * 4
        )
        
        print("âœ“ å‹ç¼©æƒé‡æ•°æ®ç»“æ„åˆ›å»ºæˆåŠŸ")
        print(f"  æ ¼å¼: {compressed_weight.format.value}")
        print(f"  åŸå§‹å½¢çŠ¶: {compressed_weight.original_shape}")
        print(f"  å‹ç¼©å¤§å°: {compressed_weight.compressed_size} å­—èŠ‚")
        print(f"  å†…å­˜ä½¿ç”¨: {compressed_weight.get_memory_usage()} å­—èŠ‚")
        
        # è®¡ç®—å‹ç¼©æ¯”
        original_size = compressed_weight.original_shape[0] * compressed_weight.original_shape[1] * 2  # float16
        compression_ratio = original_size / compressed_weight.get_memory_usage()
        print(f"  å‹ç¼©æ¯”: {compression_ratio:.2f}x")
        
        return True
        
    except Exception as e:
        print(f"âœ— å‹ç¼©æƒé‡æ•°æ®ç»“æ„æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_mixed_precision_loader():
    """æµ‹è¯•çœŸæ­£çš„æ··åˆç²¾åº¦åŠ è½½å™¨"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•çœŸæ­£çš„æ··åˆç²¾åº¦åŠ è½½å™¨")
    print("=" * 60)
    
    try:
        from sglang.srt.configs.model_config import ModelConfig
        from sglang.srt.model_loader.true_mixed_precision_loader import (
            TrueMixedPrecisionLoader,
            TrueMixedPrecisionConfig
        )
        
        # åˆ›å»ºæ··åˆç²¾åº¦é…ç½®
        mixed_precision_config = TrueMixedPrecisionConfig(
            fp16_path="/dcar-vepfs-trans-models/Qwen3-30B-A3B",
            fp8_path="/dcar-vepfs-trans-models/Qwen3-30B-A3B-FP8",
            gptq_int4_path="/dcar-vepfs-trans-models/Qwen3-30B-A3B-GPTQ-Int4",
            weight_mapping={
                "model.layers.0.self_attn.q_proj.weight": "fp16",
                "model.layers.0.mlp.experts.0.up_proj.weight": "gptq_int4",
                "model.layers.0.mlp.experts.1.up_proj.weight": "fp8"
            }
        )
        
        # åˆ›å»ºæ¨¡å‹é…ç½®
        model_config = ModelConfig(
            model_path="/dcar-vepfs-trans-models/Qwen3-30B-A3B",
            dtype="auto",
            trust_remote_code=True
        )
        
        # åˆ›å»ºçœŸæ­£çš„æ··åˆç²¾åº¦åŠ è½½å™¨
        loader = TrueMixedPrecisionLoader(model_config, mixed_precision_config)
        
        print("âœ“ çœŸæ­£çš„æ··åˆç²¾åº¦åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        print(f"  æƒé‡æ˜ å°„æ•°é‡: {len(mixed_precision_config.weight_mapping)}")
        print(f"  FP16è·¯å¾„: {mixed_precision_config.fp16_path}")
        print(f"  GPTQ-Int4è·¯å¾„: {mixed_precision_config.gptq_int4_path}")
        print(f"  AWQ-Int4è·¯å¾„: {mixed_precision_config.awq_int4_path}")
        
        return True
        
    except Exception as e:
        print(f"âœ— çœŸæ­£çš„æ··åˆç²¾åº¦åŠ è½½å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_mixed_precision_linear():
    """æµ‹è¯•æ··åˆç²¾åº¦çº¿æ€§å±‚"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ··åˆç²¾åº¦çº¿æ€§å±‚")
    print("=" * 60)
    
    try:
        from sglang.srt.layers.mixed_precision_linear import MixedPrecisionLinear
        
        # åˆ›å»ºæ··åˆç²¾åº¦çº¿æ€§å±‚
        linear_layer = MixedPrecisionLinear(
            in_features=768,
            out_features=2048,
            bias=True,
            weight_name="test.weight",
            use_cache=True
        )
        
        print("âœ“ æ··åˆç²¾åº¦çº¿æ€§å±‚åˆ›å»ºæˆåŠŸ")
        print(f"  è¾“å…¥ç‰¹å¾: {linear_layer.in_features}")
        print(f"  è¾“å‡ºç‰¹å¾: {linear_layer.out_features}")
        print(f"  æƒé‡åç§°: {linear_layer.weight_name}")
        print(f"  ä½¿ç”¨ç¼“å­˜: {linear_layer.use_cache}")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­ï¼ˆä¼šä½¿ç”¨é›¶æƒé‡ï¼Œå› ä¸ºæ²¡æœ‰çœŸå®çš„å‹ç¼©æƒé‡ï¼‰
        input_tensor = torch.randn(2, 768, dtype=torch.float16)
        output = linear_layer(input_tensor)
        
        print(f"  è¾“å…¥å½¢çŠ¶: {input_tensor.shape}")
        print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print("  å‰å‘ä¼ æ’­æˆåŠŸï¼ˆä½¿ç”¨é›¶æƒé‡ï¼‰")
        
        return True
        
    except Exception as e:
        print(f"âœ— æ··åˆç²¾åº¦çº¿æ€§å±‚æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_memory_savings():
    """æµ‹è¯•å†…å­˜èŠ‚çœæ•ˆæœ"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•å†…å­˜èŠ‚çœæ•ˆæœ")
    print("=" * 60)
    
    try:
        # æ¨¡æ‹Ÿä¸åŒæ ¼å¼çš„å†…å­˜ä½¿ç”¨
        formats = {
            'fp16': {'element_size': 2, 'compression_ratio': 1.0},
            'fp8': {'element_size': 1, 'compression_ratio': 2.0},
            'gptq_int4': {'element_size': 0.5, 'compression_ratio': 4.0},
        }
        
        # æ¨¡æ‹Ÿæƒé‡åˆ†å¸ƒ
        weight_distribution = {
            'fp16': 0.3,    # 30% ä½¿ç”¨FP16
            'fp8': 0.2,     # 20% ä½¿ç”¨FP8
            'gptq_int4': 0.1,  # 10% ä½¿ç”¨GPTQ-Int4
        }
        
        # å‡è®¾æ¨¡å‹æ€»å¤§å°
        total_model_size_mb = 1000  # 1GB
        
        # è®¡ç®—æ··åˆç²¾åº¦åçš„å†…å­˜ä½¿ç”¨
        mixed_precision_size_mb = 0
        for format_name, ratio in weight_distribution.items():
            format_info = formats[format_name]
            size_mb = total_model_size_mb * ratio / format_info['compression_ratio']
            mixed_precision_size_mb += size_mb
        
        # è®¡ç®—èŠ‚çœçš„å†…å­˜
        memory_saved_mb = total_model_size_mb - mixed_precision_size_mb
        memory_saved_percent = (memory_saved_mb / total_model_size_mb) * 100
        
        print("âœ“ å†…å­˜èŠ‚çœæ•ˆæœè®¡ç®—æˆåŠŸ")
        print(f"  åŸå§‹æ¨¡å‹å¤§å°: {total_model_size_mb}MB")
        print(f"  æ··åˆç²¾åº¦åå¤§å°: {mixed_precision_size_mb:.2f}MB")
        print(f"  èŠ‚çœå†…å­˜: {memory_saved_mb:.2f}MB ({memory_saved_percent:.1f}%)")
        
        print("\n  æƒé‡åˆ†å¸ƒ:")
        for format_name, ratio in weight_distribution.items():
            format_info = formats[format_name]
            size_mb = total_model_size_mb * ratio / format_info['compression_ratio']
            print(f"    {format_name}: {ratio*100:.0f}% -> {size_mb:.2f}MB")
        
        return True
        
    except Exception as e:
        print(f"âœ— å†…å­˜èŠ‚çœæ•ˆæœæµ‹è¯•å¤±è´¥: {e}")
        return False


def create_test_config():
    """åˆ›å»ºæµ‹è¯•é…ç½®æ–‡ä»¶"""
    print("\n" + "=" * 60)
    print("åˆ›å»ºæµ‹è¯•é…ç½®æ–‡ä»¶")
    print("=" * 60)
    
    try:
        # åˆ›å»ºæµ‹è¯•é…ç½®
        test_config = {
            "mixed_precision": {
                "fp16_path": "/dcar-vepfs-trans-models/Qwen3-30B-A3B",
                "fp8_path": "/dcar-vepfs-trans-models/Qwen3-30B-A3B-FP8",
                "gptq_int4_path": "/dcar-vepfs-trans-models/Qwen3-30B-A3B-GPTQ-Int4",
                "weight_mapping": {
                    "model.layers.0.self_attn.q_proj.weight": "fp16",
                    "model.layers.0.mlp.experts.0.up_proj.weight": "gptq_int4",
                    "model.layers.0.mlp.experts.1.up_proj.weight": "fp8",
                }
            }
        }
        
        # ä¿å­˜é…ç½®æ–‡ä»¶
        config_path = "test_mixed_precision_config.yaml"
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(test_config, f, default_flow_style=False, allow_unicode=True)
        
        print(f"âœ“ æµ‹è¯•é…ç½®æ–‡ä»¶åˆ›å»ºæˆåŠŸ: {config_path}")
        print(f"  æ”¯æŒçš„æ ¼å¼: FP16, FP8, Int8, Int4, GPTQ-Int4, AWQ-Int4")
        print(f"  æƒé‡æ˜ å°„æ•°é‡: {len(test_config['mixed_precision']['weight_mapping'])}")
        
        return config_path
        
    except Exception as e:
        print(f"âœ— æµ‹è¯•é…ç½®æ–‡ä»¶åˆ›å»ºå¤±è´¥: {e}")
        return None


def main():
    """ä¸»å‡½æ•°"""
    print("çœŸæ­£æ··åˆç²¾åº¦åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        ("å‹ç¼©æƒé‡æ•°æ®ç»“æ„", test_compressed_weight_structures),
        ("çœŸæ­£çš„æ··åˆç²¾åº¦åŠ è½½å™¨", test_mixed_precision_loader),
        ("æ··åˆç²¾åº¦çº¿æ€§å±‚", test_mixed_precision_linear),
        ("å†…å­˜èŠ‚çœæ•ˆæœ", test_memory_savings),
    ]
    
    results = []
    for test_name, test_func in tests:
        print(f"\nè¿è¡Œæµ‹è¯•: {test_name}")
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"âœ— æµ‹è¯•å¼‚å¸¸: {e}")
            results.append((test_name, False))
    
    # åˆ›å»ºæµ‹è¯•é…ç½®æ–‡ä»¶
    config_path = create_test_config()
    
    # æ˜¾ç¤ºç»“æœ
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)
    
    for test_name, result in results:
        status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
        print(f"{test_name}: {status}")
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    print(f"\næ€»è®¡: {passed}/{total} ä¸ªæµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰çœŸæ­£æ··åˆç²¾åº¦æµ‹è¯•é€šè¿‡ï¼")
        print("\næ ¸å¿ƒç‰¹æ€§:")
        print("1. å¤šç§é‡åŒ–æ ¼å¼å…±å­˜ï¼ˆFP16, FP8, GPTQ-Int4ï¼‰")
        print("2. ä¿æŒå‹ç¼©æ ¼å¼ï¼Œä¸é¢„å…ˆåé‡åŒ–")
        print("3. åŠ¨æ€åé‡åŒ–ï¼ŒæŒ‰éœ€å¤„ç†")
        print("4. çœŸæ­£çš„å†…å­˜èŠ‚çœï¼Œä¸æ˜¯æ ¼å¼è½¬æ¢")
        print("5. æ”¯æŒæƒé‡ç¼“å­˜ï¼Œæé«˜æ¨ç†æ•ˆç‡")
        
        if config_path:
            print(f"\næµ‹è¯•é…ç½®æ–‡ä»¶: {config_path}")
            print("æ‚¨å¯ä»¥ä½¿ç”¨æ­¤é…ç½®æ–‡ä»¶æµ‹è¯•çœŸæ­£çš„æ··åˆç²¾åº¦åŠŸèƒ½")
    else:
        print("âš  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚")
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    if config_path and os.path.exists(config_path):
        try:
            os.remove(config_path)
            print(f"\næ¸…ç†æµ‹è¯•æ–‡ä»¶: {config_path}")
        except:
            pass


if __name__ == "__main__":
    main()
