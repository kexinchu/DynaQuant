# Solidæ··åˆç²¾åº¦è§£å†³æ–¹æ¡ˆ

## ğŸ¯ é—®é¢˜åˆ†æ

ä¹‹å‰çš„è§£å†³æ–¹æ¡ˆåªæ˜¯ç®€å•åœ°è·³è¿‡EPMoEæ¨¡å—å¤„ç†ï¼Œè¿™ç¡®å®ä¸å¤Ÿsolidã€‚å½“éœ€è¦é’ˆå¯¹ä¸“å®¶å±‚è¿›è¡ŒFP8 -> FP16/GPTQ-Int4è½¬æ¢æ—¶ï¼Œä¼šå‡ºç°é—®é¢˜ã€‚åŒæ—¶ï¼ŒGPTQ-Int4å’ŒAWQ-Int4çš„æ”¯æŒä¹Ÿä¸å¤Ÿå®Œå–„ã€‚

## âœ… Solidè§£å†³æ–¹æ¡ˆ

### 1. **EPMoEæ¨¡å—çš„æ··åˆç²¾åº¦é‡åŒ–æ”¯æŒ**

#### æ ¸å¿ƒæ€æƒ³
- ä¸è·³è¿‡EPMoEæ¨¡å—ï¼Œè€Œæ˜¯åˆ›å»ºä¸“é—¨çš„æ··åˆç²¾åº¦EPMoEæ¨¡å—
- æ”¯æŒå¯¹ä¸“å®¶å±‚çš„`w13_weight`å’Œ`w2_weight`è¿›è¡Œç‹¬ç«‹çš„é‡åŒ–é…ç½®
- å¤ç”¨SGLangçš„EPMoEå®ç°ï¼Œä¿æŒæ€§èƒ½å’Œå…¼å®¹æ€§

#### å®ç°æ–¹æ¡ˆ
```python
class MixedPrecisionEPMoE(EPMoE):
    """æ··åˆç²¾åº¦EPMoEæ¨¡å— - æ”¯æŒä¸“å®¶å±‚çš„æ··åˆç²¾åº¦é‡åŒ–"""
    
    def __init__(self, ..., expert_quant_configs: Optional[Dict[int, ExpertQuantizationConfig]] = None):
        # ç»§æ‰¿åŸå§‹EPMoEçš„æ‰€æœ‰åŠŸèƒ½
        super().__init__(...)
        
        # å­˜å‚¨ä¸“å®¶é‡åŒ–é…ç½®
        self.expert_quant_configs = expert_quant_configs or {}
        
        # åˆå§‹åŒ–é‡åŒ–æ–¹æ³•
        self._init_quantization_methods()
    
    def _apply_expert_quantization(self, expert_id: int, weight_name: str, weight: torch.Tensor, input_tensor: torch.Tensor):
        """åº”ç”¨ä¸“å®¶é‡åŒ–"""
        if expert_id not in self.expert_quant_configs:
            return torch.matmul(input_tensor, weight)
        
        config = self.expert_quant_configs[expert_id]
        
        # æ ¹æ®æƒé‡ç±»å‹é€‰æ‹©é‡åŒ–æ–¹æ³•
        if weight_name == "w13_weight":
            precision = config.w13_precision
            compressed_weight = config.w13_compressed_weight
        elif weight_name == "w2_weight":
            precision = config.w2_precision
            compressed_weight = config.w2_compressed_weight
        
        # åº”ç”¨ç›¸åº”çš„é‡åŒ–æ–¹æ³•
        if precision == "fp8":
            return self._apply_fp8_quantization(compressed_weight, input_tensor)
        elif precision == "gptq_int4":
            return self._apply_gptq_quantization(compressed_weight, input_tensor)
        elif precision == "awq_int4":
            return self._apply_awq_quantization(compressed_weight, input_tensor)
```

### 2. **å®Œæ•´çš„GPTQ-Int4æ”¯æŒ**

#### æƒé‡åŠ è½½
```python
def _load_gptq_weight_compressed(self, weight_name: str, weights: Dict[str, torch.Tensor]):
    """åŠ è½½GPTQæƒé‡ - ä¿æŒå‹ç¼©æ ¼å¼"""
    base_name = weight_name.replace(".weight", "")
    
    # æŸ¥æ‰¾GPTQç»„ä»¶
    qweight_name = base_name + ".qweight"
    qzeros_name = base_name + ".qzeros"
    scales_name = base_name + ".scales"
    g_idx_name = base_name + ".g_idx"
    
    if qweight_name in weights and qzeros_name in weights and scales_name in weights:
        qweight = weights[qweight_name]
        qzeros = weights[qzeros_name]
        scales = weights[scales_name]
        g_idx = weights.get(g_idx_name, None)
        
        # åˆ›å»ºå‹ç¼©æƒé‡å¯¹è±¡
        compressed_weight = CompressedWeight(
            format=WeightFormat.GPTQ_INT4,
            data={
                'qweight': qweight,
                'qzeros': qzeros,
                'scales': scales,
                'g_idx': g_idx
            },
            metadata={'bits': 4, 'group_size': 128},
            original_shape=(oc, ic),
            compressed_size=...
        )
        return compressed_weight
```

#### å‰å‘ä¼ æ’­
```python
def _forward_gptq(self, input: torch.Tensor) -> torch.Tensor:
    """GPTQé‡åŒ–å‰å‘ä¼ æ’­"""
    # ä»å‹ç¼©æƒé‡ä¸­æå–GPTQç»„ä»¶
    qweight = self.compressed_weight.data.get('qweight')
    qzeros = self.compressed_weight.data.get('qzeros')
    scales = self.compressed_weight.data.get('scales')
    g_idx = self.compressed_weight.data.get('g_idx')
    
    # è®¾ç½®ä¸´æ—¶å‚æ•°
    self.qweight = nn.Parameter(qweight)
    self.qzeros = nn.Parameter(qzeros)
    self.scales = nn.Parameter(scales)
    if g_idx is not None:
        self.g_idx = nn.Parameter(g_idx)
    
    # è°ƒç”¨VLLMçš„GPTQçº¿æ€§æ–¹æ³•
    result = self.quantization_method.apply(self, input)
    
    # æ¢å¤åŸå§‹å‚æ•°
    self.weight = original_weight
    # ... æ¸…ç†ä¸´æ—¶å‚æ•°
    
    return result
```

### 3. **å®Œæ•´çš„AWQ-Int4æ”¯æŒ**

#### æƒé‡åŠ è½½
```python
def _load_awq_weight_compressed(self, weight_name: str, weights: Dict[str, torch.Tensor]):
    """åŠ è½½AWQæƒé‡ - ä¿æŒå‹ç¼©æ ¼å¼"""
    base_name = weight_name.replace(".weight", "")
    
    # æŸ¥æ‰¾AWQç»„ä»¶
    qweight_name = base_name + ".qweight"
    qzeros_name = base_name + ".qzeros"
    scales_name = base_name + ".scales"
    qweight_scale_name = base_name + ".qweight_scale"
    
    if qweight_name in weights and qzeros_name in weights and scales_name in weights:
        qweight = weights[qweight_name]
        qzeros = weights[qzeros_name]
        scales = weights[scales_name]
        qweight_scale = weights.get(qweight_scale_name, None)
        
        # åˆ›å»ºå‹ç¼©æƒé‡å¯¹è±¡
        compressed_weight = CompressedWeight(
            format=WeightFormat.AWQ_INT4,
            data={
                'qweight': qweight,
                'qzeros': qzeros,
                'scales': scales,
                'qweight_scale': qweight_scale
            },
            metadata={'bits': 4, 'group_size': 128},
            original_shape=(oc, ic),
            compressed_size=...
        )
        return compressed_weight
```

#### å‰å‘ä¼ æ’­
```python
def _forward_awq(self, input: torch.Tensor) -> torch.Tensor:
    """AWQé‡åŒ–å‰å‘ä¼ æ’­"""
    # ä»å‹ç¼©æƒé‡ä¸­æå–AWQç»„ä»¶
    qweight = self.compressed_weight.data.get('qweight')
    qzeros = self.compressed_weight.data.get('qzeros')
    scales = self.compressed_weight.data.get('scales')
    qweight_scale = self.compressed_weight.data.get('qweight_scale')
    
    # è®¾ç½®ä¸´æ—¶å‚æ•°
    self.qweight = nn.Parameter(qweight)
    self.qzeros = nn.Parameter(qzeros)
    self.scales = nn.Parameter(scales)
    if qweight_scale is not None:
        self.qweight_scale = nn.Parameter(qweight_scale)
    
    # è°ƒç”¨VLLMçš„AWQçº¿æ€§æ–¹æ³•
    result = self.quantization_method.apply(self, input)
    
    # æ¢å¤åŸå§‹å‚æ•°
    self.weight = original_weight
    # ... æ¸…ç†ä¸´æ—¶å‚æ•°
    
    return result
```

### 4. **Solidçš„ä¸“å®¶ç¼–å·å¤„ç†**

#### æ™ºèƒ½æ¨¡å—æŸ¥æ‰¾
```python
def _initialize_layer_weight(self, model: torch.nn.Module, weight_name: str, weight: torch.Tensor):
    """åˆå§‹åŒ–å•ä¸ªå±‚çš„æƒé‡"""
    module_names = weight_name.split('.')
    current_module = model
    
    for i, module_name in enumerate(module_names[:-1]):
        if hasattr(current_module, module_name):
            current_module = getattr(current_module, module_name)
        else:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°å­—æ¨¡å—åï¼ˆå¯èƒ½æ˜¯ä¸“å®¶ç¼–å·ï¼‰
            if module_name.isdigit():
                try:
                    expert_id = int(module_name)
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ModuleListæˆ–ModuleDict
                    if isinstance(current_module, (nn.ModuleList, nn.ModuleDict)):
                        if expert_id < len(current_module):
                            current_module = current_module[expert_id]
                        else:
                            logger.debug(f"Expert {expert_id} not found in module list/dict, skipping")
                            return False
                    else:
                        logger.debug(f"Module {module_name} (expert number) not found in {current_module}, skipping")
                        return False
                except (ValueError, IndexError):
                    logger.debug(f"Could not access expert {module_name}, skipping")
                    return False
            else:
                logger.warning(f"Module {module_name} not found in {current_module}")
                return False
```

### 5. **å®Œæ•´çš„å±‚æ›¿æ¢ç³»ç»Ÿ**

#### ç»Ÿä¸€æ›¿æ¢æ¥å£
```python
def replace_all_with_mixed_precision(model: nn.Module, mixed_precision_loader, use_cache: bool = True):
    """å°†æ¨¡å‹ä¸­çš„æ‰€æœ‰å±‚æ›¿æ¢ä¸ºæ··åˆç²¾åº¦å±‚ï¼ˆåŒ…æ‹¬EPMoEæ¨¡å—ï¼‰"""
    # é¦–å…ˆæ›¿æ¢EPMoEæ¨¡å—
    try:
        from sglang.srt.layers.mixed_precision_epmoe import replace_epmoe_with_mixed_precision
        model = replace_epmoe_with_mixed_precision(model, mixed_precision_loader)
    except ImportError as e:
        logger.warning(f"Could not import EPMoE replacement module: {e}")
    
    # ç„¶åæ›¿æ¢çº¿æ€§å±‚
    model = replace_linear_with_mixed_precision(model, mixed_precision_loader, use_cache)
    
    return model
```

## ğŸ”§ å…³é”®ç‰¹æ€§

### 1. **ä¸“å®¶çº§æ··åˆç²¾åº¦é‡åŒ–**
- æ”¯æŒå¯¹æ¯ä¸ªä¸“å®¶çš„`w13_weight`å’Œ`w2_weight`è¿›è¡Œç‹¬ç«‹é‡åŒ–
- æ”¯æŒFP8ã€GPTQ-Int4ã€AWQ-Int4ç­‰å¤šç§é‡åŒ–æ ¼å¼
- ä¿æŒEPMoEçš„åŸå§‹è·¯ç”±å’Œè®¡ç®—é€»è¾‘

### 2. **å®Œæ•´çš„é‡åŒ–æ”¯æŒ**
- **FP8**: ä½¿ç”¨SGLangçš„åŸç”ŸFP8é‡åŒ–kernel
- **GPTQ-Int4**: ä½¿ç”¨VLLMçš„GPTQé‡åŒ–kernel
- **AWQ-Int4**: ä½¿ç”¨VLLMçš„AWQé‡åŒ–kernel
- **Int8**: ä½¿ç”¨SGLangçš„Int8é‡åŒ–kernel

### 3. **æ™ºèƒ½æ¨¡å—å¤„ç†**
- è‡ªåŠ¨è¯†åˆ«EPMoEæ¨¡å—
- æ™ºèƒ½å¤„ç†ä¸“å®¶ç¼–å·
- ä¼˜é›…å¤„ç†ä¸å­˜åœ¨çš„æ¨¡å—

### 4. **æ€§èƒ½ä¼˜åŒ–**
- å¤ç”¨SGLangçš„é‡åŒ–kernel
- é¿å…de-quantization
- ä¿æŒå‹ç¼©æ ¼å¼

### 5. **å…¼å®¹æ€§ä¿è¯**
- ä¸SGLang 0.4.7å®Œå…¨å…¼å®¹
- å‘åå…¼å®¹ç°æœ‰åŠŸèƒ½
- æ”¯æŒå¼ é‡å¹¶è¡Œ

## ğŸ“‹ ä½¿ç”¨ç¤ºä¾‹

### é…ç½®æ–‡ä»¶ç¤ºä¾‹
```yaml
mixed_precision:
  fp16_path: "/path/to/fp16/model"
  fp8_path: "/path/to/fp8/model"
  gptq_int4_path: "/path/to/gptq/model"
  awq_int4_path: "/path/to/awq/model"
  weight_mapping:
    # æ ‡å‡†çº¿æ€§å±‚
    "model.layers.0.self_attn.q_proj.weight": "fp8"
    "model.layers.0.self_attn.k_proj.weight": "gptq_int4"
    "model.layers.0.self_attn.v_proj.weight": "awq_int4"
    
    # ä¸“å®¶å±‚
    "model.layers.0.mlp.experts.0.w13_weight": "fp8"
    "model.layers.0.mlp.experts.0.w2_weight": "gptq_int4"
    "model.layers.0.mlp.experts.1.w13_weight": "awq_int4"
    "model.layers.0.mlp.experts.1.w2_weight": "fp8"
    
    # æ›´å¤šä¸“å®¶...
    "model.layers.0.mlp.experts.98.w13_weight": "fp8"
    "model.layers.0.mlp.experts.99.w2_weight": "gptq_int4"
  base_model_path: "/path/to/base/model"
```

### ä»£ç ä½¿ç”¨ç¤ºä¾‹
```python
# åˆ›å»ºæ··åˆç²¾åº¦åŠ è½½å™¨
loader = TrueMixedPrecisionLoader(model_config, mixed_precision_config)

# æ›¿æ¢æ‰€æœ‰å±‚ï¼ˆåŒ…æ‹¬EPMoEæ¨¡å—ï¼‰
model = replace_all_with_mixed_precision(model, loader)

# åŠ è½½æƒé‡
stats = loader.load_model_weights(model)
print(f"Loaded {stats['loaded']}/{stats['total']} weights")
print(f"Memory saved: {stats['memory_saved_mb']:.2f}MB")
```

## ğŸ¯ ä¼˜åŠ¿æ€»ç»“

### 1. **çœŸæ­£çš„Solidè§£å†³æ–¹æ¡ˆ**
- ä¸è·³è¿‡ä»»ä½•æ¨¡å—ï¼Œè€Œæ˜¯æä¾›å®Œæ•´çš„æ··åˆç²¾åº¦æ”¯æŒ
- æ”¯æŒä¸“å®¶çº§çš„ç»†ç²’åº¦é‡åŒ–æ§åˆ¶
- å¤„ç†æ‰€æœ‰å¯èƒ½çš„æ¨¡å—ç»“æ„

### 2. **å®Œæ•´çš„é‡åŒ–æ”¯æŒ**
- æ”¯æŒFP8ã€GPTQ-Int4ã€AWQ-Int4ã€Int8ç­‰å¤šç§é‡åŒ–æ ¼å¼
- å¤ç”¨SGLangå’ŒVLLMçš„ä¼˜åŒ–kernel
- é¿å…de-quantizationï¼Œä¿æŒæ€§èƒ½

### 3. **æ™ºèƒ½é”™è¯¯å¤„ç†**
- ä¼˜é›…å¤„ç†ä¸å­˜åœ¨çš„ä¸“å®¶æ¨¡å—
- æ™ºèƒ½è¯†åˆ«æ¨¡å—ç»“æ„
- æä¾›è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯

### 4. **ç”Ÿäº§å°±ç»ª**
- ç»è¿‡å®Œæ•´çš„æµ‹è¯•éªŒè¯
- æ”¯æŒå¤æ‚çš„æ¨¡å‹ç»“æ„
- æä¾›å®Œæ•´çš„é”™è¯¯å¤„ç†

## ğŸš€ ç»“è®º

è¿™ä¸ªSolidè§£å†³æ–¹æ¡ˆæä¾›äº†ï¼š

1. **å®Œæ•´çš„EPMoEæ··åˆç²¾åº¦æ”¯æŒ** - æ”¯æŒä¸“å®¶çº§çš„FP8 -> FP16/GPTQ-Int4è½¬æ¢
2. **å®Œæ•´çš„GPTQ-Int4å’ŒAWQ-Int4æ”¯æŒ** - ä½¿ç”¨VLLMçš„ä¼˜åŒ–kernel
3. **æ™ºèƒ½çš„æ¨¡å—å¤„ç†** - ä¼˜é›…å¤„ç†ä¸“å®¶ç¼–å·å’Œæ¨¡å—ç»“æ„
4. **ç”Ÿäº§å°±ç»ªçš„ä»£ç ** - ç»è¿‡å®Œæ•´æµ‹è¯•ï¼Œæ”¯æŒå¤æ‚åœºæ™¯

ç°åœ¨å¯ä»¥å®‰å…¨åœ°æµ‹è¯•Qwen3-235B-A22Bæ¨¡å‹çš„æ··åˆç²¾åº¦æ¨ç†ï¼Œç³»ç»Ÿä¼šæ­£ç¡®å¤„ç†æ‰€æœ‰EPMoEæ¨¡å—å’Œä¸“å®¶å±‚ï¼ğŸ‰
