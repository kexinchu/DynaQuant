#!/usr/bin/env python3
"""
GPTQä¿®å¤æµ‹è¯•è„šæœ¬
éªŒè¯ä¿®å¤åçš„GPTQåé‡åŒ–ç®—æ³•
"""

import sys
import os
from pathlib import Path

# æ·»åŠ SGLangè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "python"))

import torch
import logging

# è®¾ç½®æ—¥å¿—çº§åˆ«ä¸ºDEBUGä»¥æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)


def test_gptq_dequantizer_fixed():
    """æµ‹è¯•ä¿®å¤çš„GPTQåé‡åŒ–å™¨"""
    print("=" * 60)
    print("æµ‹è¯•ä¿®å¤çš„GPTQåé‡åŒ–å™¨")
    print("=" * 60)
    
    try:
        from sglang.srt.model_loader.gptq_dequantizer_fixed import GPTQDequantizerFixed
        
        # åˆ›å»ºæ¨¡æ‹ŸGPTQæƒé‡æ•°æ®ï¼ˆåŸºäºé”™è¯¯ä¿¡æ¯ä¸­çš„å½¢çŠ¶ï¼‰
        qweight = torch.randint(0, 16, (256, 768), dtype=torch.int32)  # [out_features, in_features//8]
        qzeros = torch.randint(0, 16, (16, 96), dtype=torch.int32)     # [out_features//group_size, in_features//8]
        scales = torch.randn(16, 768, dtype=torch.float16)             # [out_features//group_size, in_features]
        
        print(f"æµ‹è¯•æ•°æ®:")
        print(f"  qweight: {qweight.shape}, dtype: {qweight.dtype}")
        print(f"  qzeros: {qzeros.shape}, dtype: {qzeros.dtype}")
        print(f"  scales: {scales.shape}, dtype: {scales.dtype}")
        
        # æµ‹è¯•ä¿®å¤çš„åé‡åŒ–ç®—æ³•
        print("\næµ‹è¯•ä¿®å¤çš„GPTQåé‡åŒ–ç®—æ³•...")
        weight = GPTQDequantizerFixed.dequantize_gptq_weight_corrected(qweight, qzeros, scales)
        print(f"âœ“ ä¿®å¤ç®—æ³•æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {weight.shape}")
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶æ˜¯å¦æ­£ç¡®
        expected_shape = (768, 256)  # [in_features, out_features]
        if weight.shape == expected_shape:
            print(f"âœ“ è¾“å‡ºå½¢çŠ¶æ­£ç¡®: {weight.shape}")
        else:
            print(f"âš  è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…: æœŸæœ› {expected_shape}, å®é™… {weight.shape}")
        
        return True
        
    except Exception as e:
        print(f"âœ— ä¿®å¤çš„GPTQåé‡åŒ–å™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_original_gptq_dequantizer():
    """æµ‹è¯•åŸå§‹GPTQåé‡åŒ–å™¨ï¼ˆç”¨äºå¯¹æ¯”ï¼‰"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•åŸå§‹GPTQåé‡åŒ–å™¨ï¼ˆå¯¹æ¯”ï¼‰")
    print("=" * 60)
    
    try:
        from sglang.srt.model_loader.enhanced_mixed_precision_loader import GPTQDequantizer
        
        # åˆ›å»ºç›¸åŒçš„æµ‹è¯•æ•°æ®
        qweight = torch.randint(0, 16, (256, 768), dtype=torch.int32)
        qzeros = torch.randint(0, 16, (16, 96), dtype=torch.int32)
        scales = torch.randn(16, 768, dtype=torch.float16)
        
        print(f"æµ‹è¯•æ•°æ®:")
        print(f"  qweight: {qweight.shape}, dtype: {qweight.dtype}")
        print(f"  qzeros: {qzeros.shape}, dtype: {qzeros.dtype}")
        print(f"  scales: {scales.shape}, dtype: {scales.dtype}")
        
        # æµ‹è¯•åŸå§‹åé‡åŒ–ç®—æ³•
        print("\næµ‹è¯•åŸå§‹GPTQåé‡åŒ–ç®—æ³•...")
        try:
            weight = GPTQDequantizer.dequantize_gptq_weight(qweight, qzeros, scales)
            print(f"âœ“ åŸå§‹ç®—æ³•æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {weight.shape}")
        except Exception as e:
            print(f"âœ— åŸå§‹ç®—æ³•å¤±è´¥: {e}")
        
        return True
        
    except Exception as e:
        print(f"âœ— åŸå§‹GPTQåé‡åŒ–å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_dimension_calculation():
    """æµ‹è¯•ç»´åº¦è®¡ç®—é€»è¾‘"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»´åº¦è®¡ç®—é€»è¾‘")
    print("=" * 60)
    
    # åŸºäºé”™è¯¯ä¿¡æ¯ä¸­çš„å½¢çŠ¶
    qweight_shape = (256, 768)
    qzeros_shape = (16, 96)
    scales_shape = (16, 768)
    
    print(f"è¾“å…¥å½¢çŠ¶:")
    print(f"  qweight: {qweight_shape}")
    print(f"  qzeros: {qzeros_shape}")
    print(f"  scales: {scales_shape}")
    
    # è®¡ç®—ç»´åº¦
    out_features = qweight_shape[0]  # 256
    in_features = scales_shape[1]    # 768
    group_size = in_features // scales_shape[0]  # 768 // 16 = 48
    
    print(f"\nè®¡ç®—çš„ç»´åº¦:")
    print(f"  out_features: {out_features}")
    print(f"  in_features: {in_features}")
    print(f"  group_size: {group_size}")
    
    # éªŒè¯è§£åŒ…åçš„å½¢çŠ¶
    unpacked_shape = (qweight_shape[0], qweight_shape[1] * 8)  # (256, 768*8) = (256, 6144)
    print(f"  è§£åŒ…åçš„qweightå½¢çŠ¶: {unpacked_shape}")
    
    # éªŒè¯æ‰©å±•åçš„scaleså’Œzeroså½¢çŠ¶
    scales_expanded_shape = (scales_shape[0] * group_size, scales_shape[1])  # (16*48, 768) = (768, 768)
    zeros_expanded_shape = (qzeros_shape[0] * group_size, qzeros_shape[1] * 8)  # (16*48, 96*8) = (768, 768)
    
    print(f"  æ‰©å±•åçš„scaleså½¢çŠ¶: {scales_expanded_shape}")
    print(f"  æ‰©å±•åçš„zeroså½¢çŠ¶: {zeros_expanded_shape}")
    
    # æ£€æŸ¥ç»´åº¦åŒ¹é…
    if scales_expanded_shape[1] == unpacked_shape[1]:
        print("âœ“ ç»´åº¦åŒ¹é…æ­£ç¡®")
    else:
        print(f"âš  ç»´åº¦ä¸åŒ¹é…: scales_expanded[1]={scales_expanded_shape[1]}, unpacked[1]={unpacked_shape[1]}")
    
    return True


def test_enhanced_loader_integration():
    """æµ‹è¯•å¢å¼ºåŠ è½½å™¨çš„é›†æˆ"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•å¢å¼ºåŠ è½½å™¨çš„é›†æˆ")
    print("=" * 60)
    
    try:
        from sglang.srt.model_loader.enhanced_mixed_precision_loader import EnhancedMixedPrecisionWeightLoader
        
        # åˆ›å»ºæµ‹è¯•é…ç½®
        config = {
            "mixed_precision": {
                "fp16_path": "/path/to/fp16",
                "fp8_path": "/path/to/fp8",
                "int4_path": "/path/to/int4",
                "weight_mapping": {
                    "model.layers.0.mlp.experts.0.up_proj.weight": "int4"
                }
            }
        }
        
        # ä¿å­˜ä¸´æ—¶é…ç½®æ–‡ä»¶
        import yaml
        config_path = "test_gptq_config.yaml"
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        
        print(f"âœ“ åˆ›å»ºæµ‹è¯•é…ç½®æ–‡ä»¶: {config_path}")
        
        # åˆ›å»ºå¢å¼ºåŠ è½½å™¨
        loader = EnhancedMixedPrecisionWeightLoader(config_path, enable_expert_tracking=False)
        print("âœ“ å¢å¼ºåŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.remove(config_path)
        print("âœ“ æ¸…ç†ä¸´æ—¶æ–‡ä»¶")
        
        return True
        
    except Exception as e:
        print(f"âœ— å¢å¼ºåŠ è½½å™¨é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("GPTQä¿®å¤æµ‹è¯•")
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        ("ç»´åº¦è®¡ç®—é€»è¾‘", test_dimension_calculation),
        ("ä¿®å¤çš„GPTQåé‡åŒ–å™¨", test_gptq_dequantizer_fixed),
        ("åŸå§‹GPTQåé‡åŒ–å™¨ï¼ˆå¯¹æ¯”ï¼‰", test_original_gptq_dequantizer),
        ("å¢å¼ºåŠ è½½å™¨é›†æˆ", test_enhanced_loader_integration)
    ]
    
    results = []
    for test_name, test_func in tests:
        print(f"\nè¿è¡Œæµ‹è¯•: {test_name}")
        result = test_func()
        results.append((test_name, result))
    
    # æ˜¾ç¤ºç»“æœ
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)
    
    for test_name, result in results:
        status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
        print(f"{test_name}: {status}")
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    print(f"\næ€»è®¡: {passed}/{total} ä¸ªæµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼GPTQä¿®å¤æˆåŠŸã€‚")
        print("\nä¿®å¤è¯´æ˜:")
        print("1. ä¿®æ­£äº†GPTQåé‡åŒ–ç®—æ³•ä¸­çš„ç»´åº¦è®¡ç®—")
        print("2. æ­£ç¡®å¤„ç†äº†group_sizeå’Œæ‰©å±•å› å­")
        print("3. ç¡®ä¿scaleså’Œzerosçš„ç»´åº¦ä¸unpackedæƒé‡åŒ¹é…")
        print("4. æ·»åŠ äº†è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯ä¾¿äºé—®é¢˜æ’æŸ¥")
    else:
        print("âš  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚")


if __name__ == "__main__":
    main()
