#!/usr/bin/env python3
"""
æµ‹è¯•å¢å¼ºçš„SGLangåŠŸèƒ½
éªŒè¯æ··åˆç²¾åº¦æƒé‡åŠ è½½å’Œä¸“å®¶æ¿€æ´»è·Ÿè¸ª
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import Dict, Any

# æ·»åŠ SGLangè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "python"))

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

from sglang.srt.enhanced_model_loader import (
    load_model_with_enhanced_features,
    get_expert_activation_stats,
    reset_expert_activation_stats,
    export_expert_activation_stats
)
from sglang.srt.model_loader.enhanced_mixed_precision_loader import (
    get_global_expert_tracker,
    GPTQDequantizer
)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def test_gptq_dequantizer():
    """æµ‹è¯•GPTQåé‡åŒ–å™¨"""
    print("=" * 50)
    print("æµ‹è¯•GPTQåé‡åŒ–å™¨")
    print("=" * 50)
    
    try:
        # åˆ›å»ºæ¨¡æ‹ŸGPTQæƒé‡æ•°æ®
        qweight = torch.randint(0, 16, (256, 768), dtype=torch.int32)
        qzeros = torch.randint(0, 16, (16, 96), dtype=torch.int32)
        scales = torch.randn(16, 768, dtype=torch.float16)
        
        print(f"æµ‹è¯•æ•°æ®:")
        print(f"  qweight: {qweight.shape}, dtype: {qweight.dtype}")
        print(f"  qzeros: {qzeros.shape}, dtype: {qzeros.dtype}")
        print(f"  scales: {scales.shape}, dtype: {scales.dtype}")
        
        # æµ‹è¯•åé‡åŒ–
        weight = GPTQDequantizer.dequantize_gptq_weight(qweight, qzeros, scales)
        print(f"âœ“ åé‡åŒ–æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {weight.shape}")
        
        return True
        
    except Exception as e:
        print(f"âœ— GPTQåé‡åŒ–å™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_enhanced_model_loader():
    """æµ‹è¯•å¢å¼ºçš„æ¨¡å‹åŠ è½½å™¨"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•å¢å¼ºçš„æ¨¡å‹åŠ è½½å™¨")
    print("=" * 50)
    
    try:
        # æ£€æŸ¥é…ç½®æ–‡ä»¶
        config_path = "mixed_precision_config.yaml"
        if os.path.exists(config_path):
            print(f"âœ“ é…ç½®æ–‡ä»¶å­˜åœ¨: {config_path}")
            
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•æ¨¡å‹
            class TestMoEModel(torch.nn.Module):
                def __init__(self):
                    super().__init__()
                    self.experts = torch.nn.ModuleList([
                        torch.nn.Linear(768, 2048) for _ in range(8)
                    ])
                    self.gate = torch.nn.Linear(768, 8)
                
                def forward(self, x):
                    gate_output = self.gate(x)
                    expert_weights = torch.softmax(gate_output, dim=-1)
                    
                    # æ¨¡æ‹Ÿä¸“å®¶æ¿€æ´»
                    expert_outputs = []
                    for i, expert in enumerate(self.experts):
                        expert_output = expert(x)
                        expert_outputs.append(expert_output * expert_weights[:, i:i+1])
                    
                    return torch.sum(torch.stack(expert_outputs), dim=0)
            
            # åˆ›å»ºæµ‹è¯•æ¨¡å‹
            model = TestMoEModel()
            print(f"âœ“ æµ‹è¯•æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")
            
            # æµ‹è¯•æ¨¡å‹åŠ è½½
            try:
                stats = load_model_with_enhanced_features(
                    model, config_path, enable_expert_tracking=True, enable_moe_tracking=True
                )
                print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
                print(f"  åŠ è½½ç»Ÿè®¡: {json.dumps(stats, indent=2)}")
                
                # æµ‹è¯•ä¸“å®¶ç»Ÿè®¡
                expert_stats = get_expert_activation_stats()
                print(f"âœ“ ä¸“å®¶ç»Ÿè®¡è·å–æˆåŠŸ: {len(expert_stats)} é¡¹")
                
                return True
                
            except Exception as e:
                print(f"âš  æ¨¡å‹åŠ è½½æµ‹è¯•å¤±è´¥ï¼ˆé¢„æœŸè¡Œä¸ºï¼Œå› ä¸ºæ–‡ä»¶ä¸å­˜åœ¨ï¼‰: {e}")
                return True  # è¿™æ˜¯é¢„æœŸçš„ï¼Œå› ä¸ºæµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨
                
        else:
            print(f"âš  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return False
            
    except Exception as e:
        print(f"âœ— å¢å¼ºæ¨¡å‹åŠ è½½å™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_expert_tracking():
    """æµ‹è¯•ä¸“å®¶æ¿€æ´»è·Ÿè¸ª"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•ä¸“å®¶æ¿€æ´»è·Ÿè¸ª")
    print("=" * 50)
    
    try:
        expert_tracker = get_global_expert_tracker()
        if expert_tracker is None:
            print("âš  æ²¡æœ‰å…¨å±€ä¸“å®¶è·Ÿè¸ªå™¨ï¼Œè·³è¿‡æµ‹è¯•")
            return True
        
        # æ¨¡æ‹Ÿä¸“å®¶æ¿€æ´»
        print("æ¨¡æ‹Ÿä¸“å®¶æ¿€æ´»...")
        for layer_id in range(3):
            for expert_id in range(4):
                expert_tracker.record_expert_activation(layer_id, expert_id, tokens_processed=10)
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        expert_stats = expert_tracker.get_expert_stats()
        print(f"âœ“ ä¸“å®¶ç»Ÿè®¡: {len(expert_stats)} ä¸ªä¸“å®¶")
        
        top_experts = expert_tracker.get_top_experts(5)
        print(f"âœ“ å‰5ä¸ªä¸“å®¶: {len(top_experts)} ä¸ª")
        
        layer_stats = expert_tracker.get_layer_stats()
        print(f"âœ“ å±‚ç»Ÿè®¡: {len(layer_stats)} å±‚")
        
        # æµ‹è¯•é‡ç½®
        expert_tracker.reset_stats()
        print("âœ“ ç»Ÿè®¡é‡ç½®æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âœ— ä¸“å®¶æ¿€æ´»è·Ÿè¸ªæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_safetensors_compatibility():
    """æµ‹è¯•safetensorså…¼å®¹æ€§"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•Safetensorså…¼å®¹æ€§")
    print("=" * 50)
    
    try:
        # æµ‹è¯•safetensorså¯¼å…¥
        try:
            from safetensors.torch import load_file, safe_open
            print("âœ“ safetensors.torchå¯¼å…¥æˆåŠŸ")
        except ImportError:
            try:
                from safetensors import load_file, safe_open
                print("âœ“ safetensorså¯¼å…¥æˆåŠŸ")
            except ImportError:
                import safetensors
                load_file = safetensors.load_file
                safe_open = safetensors.safe_open
                print("âœ“ safetensorså…¼å®¹æ€§å¯¼å…¥æˆåŠŸ")
        
        print("âœ“ Safetensorså…¼å®¹æ€§æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âœ— Safetensorså…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
        return False


def create_test_config():
    """åˆ›å»ºæµ‹è¯•é…ç½®æ–‡ä»¶"""
    print("\n" + "=" * 50)
    print("åˆ›å»ºæµ‹è¯•é…ç½®æ–‡ä»¶")
    print("=" * 50)
    
    try:
        config = {
            "mixed_precision": {
                "fp16_path": "/path/to/fp16/weights",
                "fp8_path": "/path/to/fp8/weights", 
                "int4_path": "/path/to/int4/weights",
                "weight_mapping": {
                    "model.layers.0.self_attn.q_proj.weight": "fp16",
                    "model.layers.0.self_attn.k_proj.weight": "fp16",
                    "model.layers.0.self_attn.v_proj.weight": "fp16",
                    "model.layers.0.self_attn.o_proj.weight": "fp16",
                    "model.layers.0.mlp.gate_proj.weight": "fp8",
                    "model.layers.0.mlp.up_proj.weight": "fp8",
                    "model.layers.0.mlp.down_proj.weight": "fp8",
                    "model.layers.0.mlp.experts.0.gate_proj.weight": "int4",
                    "model.layers.0.mlp.experts.0.up_proj.weight": "int4",
                    "model.layers.0.mlp.experts.0.down_proj.weight": "int4"
                }
            },
            "inference": {
                "max_seq_length": 4096,
                "max_batch_size": 32,
                "dtype": "bfloat16",
                "device_map": "auto"
            },
            "server": {
                "host": "127.0.0.1",
                "port": 8080,
                "max_workers": 4
            }
        }
        
        with open("test_mixed_precision_config.yaml", "w", encoding="utf-8") as f:
            import yaml
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        
        print("âœ“ æµ‹è¯•é…ç½®æ–‡ä»¶åˆ›å»ºæˆåŠŸ: test_mixed_precision_config.yaml")
        return True
        
    except Exception as e:
        print(f"âœ— åˆ›å»ºæµ‹è¯•é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("å¢å¼ºçš„SGLangåŠŸèƒ½æµ‹è¯•")
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        ("Safetensorså…¼å®¹æ€§", test_safetensors_compatibility),
        ("GPTQåé‡åŒ–å™¨", test_gptq_dequantizer),
        ("åˆ›å»ºæµ‹è¯•é…ç½®", create_test_config),
        ("å¢å¼ºæ¨¡å‹åŠ è½½å™¨", test_enhanced_model_loader),
        ("ä¸“å®¶æ¿€æ´»è·Ÿè¸ª", test_expert_tracking)
    ]
    
    results = []
    for test_name, test_func in tests:
        print(f"\nè¿è¡Œæµ‹è¯•: {test_name}")
        result = test_func()
        results.append((test_name, result))
    
    # æ˜¾ç¤ºç»“æœ
    print("\n" + "=" * 50)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 50)
    
    for test_name, result in results:
        status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
        print(f"{test_name}: {status}")
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    print(f"\næ€»è®¡: {passed}/{total} ä¸ªæµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¢å¼ºåŠŸèƒ½é›†æˆæˆåŠŸã€‚")
        print("\nä¸‹ä¸€æ­¥:")
        print("1. é…ç½®å®é™…çš„æ¨¡å‹è·¯å¾„å’Œæƒé‡æ–‡ä»¶")
        print("2. è¿è¡Œ launch_enhanced_server.py å¯åŠ¨å¢å¼ºæœåŠ¡å™¨")
        print("3. ä½¿ç”¨APIè¿›è¡Œæ–‡æœ¬ç”Ÿæˆå’Œä¸“å®¶ç»Ÿè®¡æŸ¥è¯¢")
    else:
        print("âš  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚")


if __name__ == "__main__":
    main()
