# 真正的混合精度配置文件
# 支持多种量化格式共存，保持压缩格式以节省GPU存储

mixed_precision:
  # 不同精度权重的路径
  fp16_path: "/dcar-vepfs-trans-models/Qwen3-30B-A3B"
  fp8_path: "/dcar-vepfs-trans-models/Qwen3-30B-A3B-FP8"
  gptq_int4_path: "/dcar-vepfs-trans-models/Qwen3-30B-A3B-GPTQ-Int4"
  
  # 权重映射配置
  # 格式: "权重名称": "精度类型"
  weight_mapping:
    # 注意力层使用FP16（保持高精度）
    "model.layers.0.self_attn.q_proj.weight": "fp16"
    "model.layers.0.self_attn.k_proj.weight": "fp16"
    "model.layers.0.self_attn.v_proj.weight": "fp16"
    "model.layers.0.self_attn.o_proj.weight": "fp16"
    
    # MLP层使用FP8（中等精度）
    "model.layers.0.mlp.gate_proj.weight": "fp8"
    "model.layers.0.mlp.up_proj.weight": "fp8"
    "model.layers.0.mlp.down_proj.weight": "fp8"
    
    # 专家层使用GPTQ-Int4（高压缩比）
    "model.layers.0.mlp.experts.0.gate_proj.weight": "gptq_int4"
    "model.layers.0.mlp.experts.0.up_proj.weight": "gptq_int4"
    "model.layers.0.mlp.experts.0.down_proj.weight": "gptq_int4"
    
# 推理配置
inference:
  max_seq_length: 4096
  max_batch_size: 32
  dtype: "bfloat16"
  device_map: "auto"
  
  # 混合精度特定配置
  mixed_precision:
    use_cache: true  # 是否使用权重缓存
    cache_size_mb: 1024  # 缓存大小限制
    enable_dynamic_dequantization: true  # 启用动态反量化

# 服务器配置
server:
  host: "127.0.0.1"
  port: 8080
  max_workers: 4
