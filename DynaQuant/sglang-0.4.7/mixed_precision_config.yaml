# 混合精度配置文件
# 支持先加载低精度模型，再替换指定层为高精度
# 支持safetensors索引文件和默认策略

mixed_precision:
  # 基础模型路径（通常是低精度模型，用于初始加载以避免OOM）
  base_model_path: "/dcar-vepfs-trans-models/Qwen3-30B-A3B-FP8"
  
  # 不同精度的模型路径
  fp16_path: "/dcar-vepfs-trans-models/Qwen3-30B-A3B"
  fp8_path: "/dcar-vepfs-trans-models/Qwen3-30B-A3B-FP8"
  gptq_int4_path: "/dcar-vepfs-trans-models/Qwen3-30B-A3B-GPTQ-Int4"
  
  # 权重映射配置 - 只需要指定专家层的精度，其他层默认使用FP16
  weight_mapping:
    # 专家层使用GPTQ-Int4（低精度，节省内存）
    # 注意：只需要指定专家层，其他层会自动使用FP16
    "model.layers.0.mlp.experts.0.gate_proj.weight": "gptq_int4"
    "model.layers.0.mlp.experts.0.up_proj.weight": "gptq_int4"
    "model.layers.0.mlp.experts.0.down_proj.weight": "gptq_int4"
    "model.layers.0.mlp.experts.1.gate_proj.weight": "gptq_int4"
    "model.layers.0.mlp.experts.1.up_proj.weight": "gptq_int4"
    "model.layers.0.mlp.experts.1.down_proj.weight": "gptq_int4"

# 加载策略配置
loading_strategy:
  # 是否启用混合精度加载
  enabled: true
  
  # 基础模型精度（用于初始加载）
  base_precision: "fp8"
  
  # 内存优化策略
  memory_optimization:
    # 是否启用压缩权重缓存
    enable_cache: true
    # 缓存大小限制（MB）
    cache_size_limit: 1024
    # 是否启用动态反量化
    enable_dynamic_dequantization: true

# 推理配置
inference:
  max_seq_length: 4096
  max_batch_size: 32
  dtype: "bfloat16"
  device_map: "auto"
  
  # 混合精度特定配置
  mixed_precision:
    use_cache: true  # 是否使用权重缓存
    cache_size_mb: 1024  # 缓存大小限制
    enable_dynamic_dequantization: true  # 启用动态反量化

# 服务器配置
server:
  host: "127.0.0.1"
  port: 8080
  max_workers: 4
