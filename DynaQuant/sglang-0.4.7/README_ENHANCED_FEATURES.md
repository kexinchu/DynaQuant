# SGLangçœŸæ­£æ··åˆç²¾åº¦é›†æˆ

æœ¬é¡¹ç›®å°†**çœŸæ­£çš„æ··åˆç²¾åº¦**åŠŸèƒ½é›†æˆåˆ°SGLang 0.4.7çš„æ ¸å¿ƒæ¶æ„ä¸­ï¼Œæ”¯æŒå¤šç§é‡åŒ–æ ¼å¼å…±å­˜ï¼Œä¿æŒå„è‡ªçš„å‹ç¼©æ ¼å¼ä»¥èŠ‚çœGPUå­˜å‚¨ï¼Œè€Œä¸æ˜¯è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼ã€‚

## ğŸ¯ æ ¸å¿ƒä¼˜åŠ¿

### 1. **çœŸæ­£çš„æ··åˆç²¾åº¦**
- **å¤šç§é‡åŒ–æ ¼å¼å…±å­˜**: FP16ã€FP8ã€Int4ã€Int8æ ¼å¼åŒæ—¶å­˜åœ¨
- **ä¿æŒå‹ç¼©æ ¼å¼**: GPTQ Int4æƒé‡ä¿æŒå‹ç¼©æ ¼å¼ï¼ŒèŠ‚çœ75%å­˜å‚¨
- **åŠ¨æ€åé‡åŒ–**: ä»…åœ¨æ¨ç†æ—¶åé‡åŒ–ï¼Œä¸é¢„å…ˆè½¬æ¢
- **çœŸæ­£çš„å†…å­˜èŠ‚çœ**: ä¸æ˜¯è™šå‡çš„æ ¼å¼è½¬æ¢ï¼Œè€Œæ˜¯çœŸæ­£çš„å‹ç¼©å­˜å‚¨

### 2. **SGLangæ·±åº¦é›†æˆ**
- **ä½¿ç”¨SGLangçš„API**: é€šè¿‡`ModelConfig`ã€`DeviceConfig`ã€`LoadConfig`ç­‰SGLangæ ¸å¿ƒé…ç½®
- **åˆ©ç”¨SGLangä¼˜åŒ–**: ä½¿ç”¨SGLangçš„é«˜æ€§èƒ½æ¨ç†å¼•æ“å’Œå†…å­˜ç®¡ç†
- **ä¿æŒAPIå…¼å®¹**: å®Œå…¨å…¼å®¹SGLangçš„ç°æœ‰APIå’ŒåŠŸèƒ½

## ğŸš€ æ ¸å¿ƒåŠŸèƒ½

### 1. çœŸæ­£çš„æ··åˆç²¾åº¦æƒé‡åŠ è½½
- **å¤šç§é‡åŒ–æ ¼å¼å…±å­˜**: FP16ã€FP8ã€Int4ã€Int8æ ¼å¼åŒæ—¶å­˜åœ¨
- **ä¿æŒå‹ç¼©æ ¼å¼**: GPTQ Int4æƒé‡ä¿æŒå‹ç¼©æ ¼å¼ï¼ŒèŠ‚çœ75%å­˜å‚¨
- **åŠ¨æ€åé‡åŒ–**: ä»…åœ¨æ¨ç†æ—¶åé‡åŒ–ï¼Œä¸é¢„å…ˆè½¬æ¢
- **æƒé‡ç¼“å­˜**: æ”¯æŒåé‡åŒ–ç»“æœç¼“å­˜ï¼Œé¿å…é‡å¤è®¡ç®—

### 2. æ··åˆç²¾åº¦çº¿æ€§å±‚
- **åŠ¨æ€æ ¼å¼å¤„ç†**: æ ¹æ®æƒé‡æ ¼å¼åŠ¨æ€é€‰æ‹©å¤„ç†æ–¹å¼
- **ç¼“å­˜ä¼˜åŒ–**: æ”¯æŒæƒé‡ç¼“å­˜ï¼Œæé«˜æ¨ç†æ•ˆç‡
- **å†…å­˜ä¼˜åŒ–**: çœŸæ­£çš„å†…å­˜èŠ‚çœï¼Œä¸æ˜¯æ ¼å¼è½¬æ¢

### 3. ä¸“å®¶æ¿€æ´»è·Ÿè¸ªï¼ˆç‹¬ç«‹ç‰ˆæœ¬ï¼‰
- **å®æ—¶ç›‘æ§**: è·Ÿè¸ªMoEæ¨¡å‹ä¸­æ¯ä¸ªä¸“å®¶çš„æ¿€æ´»æƒ…å†µ
- **ç»Ÿè®¡åˆ†æ**: æä¾›è¯¦ç»†çš„ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡
- **æ€§èƒ½åˆ†æ**: åˆ†æä¸“å®¶åˆ©ç”¨ç‡å’Œè´Ÿè½½åˆ†å¸ƒ
- **æ•°æ®å¯¼å‡º**: æ”¯æŒç»Ÿè®¡æ•°æ®çš„å¯¼å‡ºå’Œåˆ†æ

## ğŸ“ æ–‡ä»¶ç»“æ„

```
sglang-0.4.7/
â”œâ”€â”€ python/sglang/srt/
â”‚   â”œâ”€â”€ model_loader/
â”‚   â”‚   â”œâ”€â”€ true_mixed_precision_loader.py      # çœŸæ­£çš„æ··åˆç²¾åº¦åŠ è½½å™¨
â”‚   â”‚   â”œâ”€â”€ sglang_mixed_precision_loader.py    # SGLangé›†æˆçš„æ··åˆç²¾åº¦åŠ è½½å™¨
â”‚   â”‚   â”œâ”€â”€ enhanced_mixed_precision_loader.py  # å¢å¼ºçš„æ··åˆç²¾åº¦åŠ è½½å™¨ï¼ˆç‹¬ç«‹ç‰ˆæœ¬ï¼‰
â”‚   â”‚   â””â”€â”€ loader.py                           # ä¿®æ”¹çš„SGLangåŠ è½½å™¨ï¼ˆé›†æˆæ··åˆç²¾åº¦ï¼‰
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â””â”€â”€ mixed_precision_linear.py           # æ··åˆç²¾åº¦çº¿æ€§å±‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ moe_tracker.py                      # MoEä¸“å®¶è·Ÿè¸ªå™¨
â”‚   â””â”€â”€ enhanced_model_loader.py                # å¢å¼ºçš„æ¨¡å‹åŠ è½½å™¨ï¼ˆç‹¬ç«‹ç‰ˆæœ¬ï¼‰
â”œâ”€â”€ launch_sglang_mixed_precision.py            # SGLangé›†æˆæœåŠ¡å™¨å¯åŠ¨è„šæœ¬
â”œâ”€â”€ test_true_mixed_precision.py                # çœŸæ­£æ··åˆç²¾åº¦åŠŸèƒ½æµ‹è¯•è„šæœ¬
â”œâ”€â”€ test_sglang_integration.py                  # SGLangé›†æˆæµ‹è¯•è„šæœ¬
â”œâ”€â”€ start_sglang_mixed_precision.sh             # SGLangé›†æˆå¯åŠ¨è„šæœ¬
â”œâ”€â”€ mixed_precision_config.yaml            # çœŸæ­£æ··åˆç²¾åº¦é…ç½®æ–‡ä»¶
â”œâ”€â”€ mixed_precision_config.yaml                 # æ··åˆç²¾åº¦é…ç½®æ–‡ä»¶
â””â”€â”€ README_ENHANCED_FEATURES.md                 # æœ¬æ–‡æ¡£
```

## ğŸ› ï¸ å®‰è£…å’Œé…ç½®

### 1. ç¯å¢ƒè¦æ±‚
```bash
# Python 3.8+
python3 --version

# å¿…è¦çš„åŒ…
pip install torch transformers safetensors pyyaml
```

### 2. é…ç½®æ–‡ä»¶
åˆ›å»ºæ··åˆç²¾åº¦é…ç½®æ–‡ä»¶ `mixed_precision_config.yaml`:

```yaml
mixed_precision:
  # ä¸åŒç²¾åº¦æƒé‡çš„è·¯å¾„
  fp16_path: "/path/to/fp16/weights"
  fp8_path: "/path/to/fp8/weights"
  int4_path: "/path/to/int4/weights"
  
  # æƒé‡æ˜ å°„é…ç½®
  weight_mapping:
    # æ³¨æ„åŠ›å±‚ä½¿ç”¨FP16
    "model.layers.0.self_attn.q_proj.weight": "fp16"
    "model.layers.0.self_attn.k_proj.weight": "fp16"
    "model.layers.0.self_attn.v_proj.weight": "fp16"
    "model.layers.0.self_attn.o_proj.weight": "fp16"
    
    # MLPå±‚ä½¿ç”¨FP8
    "model.layers.0.mlp.gate_proj.weight": "fp8"
    "model.layers.0.mlp.up_proj.weight": "fp8"
    "model.layers.0.mlp.down_proj.weight": "fp8"
    
    # ä¸“å®¶å±‚ä½¿ç”¨Int4
    "model.layers.0.mlp.experts.0.gate_proj.weight": "int4"
    "model.layers.0.mlp.experts.0.up_proj.weight": "int4"
    "model.layers.0.mlp.experts.0.down_proj.weight": "int4"

inference:
  max_seq_length: 4096
  max_batch_size: 32
  dtype: "bfloat16"
  device_map: "auto"

server:
  host: "127.0.0.1"
  port: 8080
  max_workers: 4
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. è¿è¡ŒçœŸæ­£æ··åˆç²¾åº¦æµ‹è¯•
```bash
# æµ‹è¯•çœŸæ­£æ··åˆç²¾åº¦åŠŸèƒ½
python3 test_true_mixed_precision.py

# æµ‹è¯•SGLangé›†æˆåŠŸèƒ½
python3 test_sglang_integration.py

# æˆ–è€…ä½¿ç”¨å¯åŠ¨è„šæœ¬æµ‹è¯•
./start_sglang_mixed_precision.sh --help
```

### 2. å¯åŠ¨çœŸæ­£æ··åˆç²¾åº¦æœåŠ¡å™¨
```bash
# ä½¿ç”¨çœŸæ­£æ··åˆç²¾åº¦é…ç½®
./start_sglang_mixed_precision.sh -m /path/to/model -c mixed_precision_config.yaml

# æˆ–è€…ç›´æ¥è¿è¡ŒPythonè„šæœ¬
python3 launch_sglang_mixed_precision.py \
  --model /path/to/model \
  --mixed-precision-config mixed_precision_config.yaml \
  --device cuda \
  --dtype auto \
  --test
```

### 3. ä½¿ç”¨çœŸæ­£æ··åˆç²¾åº¦API
```python
# ä½¿ç”¨çœŸæ­£æ··åˆç²¾åº¦åŠŸèƒ½
from sglang.srt.configs.model_config import ModelConfig
from sglang.srt.configs.device_config import DeviceConfig
from sglang.srt.configs.load_config import LoadConfig, LoadFormat
from sglang.srt.model_loader.loader import DefaultModelLoader

# åˆ›å»ºSGLangé…ç½®
model_config = ModelConfig(
    model_path="/path/to/model",
    mixed_precision_config="mixed_precision_config.yaml",
    dtype="auto",
    trust_remote_code=True
)

device_config = DeviceConfig(device="cuda")
load_config = LoadConfig(load_format=LoadFormat.AUTO)

# ä½¿ç”¨SGLangåŠ è½½å™¨åŠ è½½æ¨¡å‹ï¼ˆè‡ªåŠ¨ä½¿ç”¨çœŸæ­£æ··åˆç²¾åº¦ï¼‰
loader = DefaultModelLoader(load_config)
model = loader.load_model(
    model_config=model_config,
    device_config=device_config
)

# è·å–çœŸæ­£æ··åˆç²¾åº¦ç»Ÿè®¡
from sglang.srt.model_loader.true_mixed_precision_loader import get_global_true_mixed_precision_loader
from sglang.srt.layers.mixed_precision_linear import get_mixed_precision_memory_stats

mixed_precision_loader = get_global_true_mixed_precision_loader()
if mixed_precision_loader:
    memory_stats = get_mixed_precision_memory_stats()
    print(f"å†…å­˜èŠ‚çœ: {memory_stats['memory_saved_mb']:.2f}MB")
    print(f"å‹ç¼©æ¯”: {memory_stats['compression_ratio']:.2f}x")
```

## ğŸ”§ SGLangé›†æˆæ¶æ„

### 1. é›†æˆæ–¹å¼
- **ç»§æ‰¿SGLangåŸºç±»**: `SGLangMixedPrecisionLoader`ç»§æ‰¿è‡ª`ModelLoader`
- **ä½¿ç”¨SGLangé…ç½®**: é€šè¿‡`ModelConfig`çš„`mixed_precision_config`å‚æ•°
- **é›†æˆåˆ°åŠ è½½æµç¨‹**: åœ¨`DefaultModelLoader.load_model()`ä¸­è‡ªåŠ¨æ£€æµ‹å’Œä½¿ç”¨
- **ä¿æŒå‘åå…¼å®¹**: ä¸å½±å“SGLangçš„ç°æœ‰åŠŸèƒ½

### 2. æ ¸å¿ƒç»„ä»¶
- **SGLangMixedPrecisionLoader**: ç»§æ‰¿SGLangçš„ModelLoaderï¼Œæ”¯æŒæ··åˆç²¾åº¦
- **SGLangGPTQDequantizer**: é›†æˆåˆ°SGLangçš„GPTQåé‡åŒ–å™¨
- **MixedPrecisionConfig**: æ··åˆç²¾åº¦é…ç½®æ•°æ®ç»“æ„
- **å…¨å±€åŠ è½½å™¨ç®¡ç†**: é€šè¿‡å…¨å±€å˜é‡ç®¡ç†æ··åˆç²¾åº¦åŠ è½½å™¨å®ä¾‹

### 3. å·¥ä½œæµç¨‹
```
1. åˆ›å»ºModelConfigï¼ŒæŒ‡å®šmixed_precision_config
2. DefaultModelLoaderæ£€æµ‹åˆ°æ··åˆç²¾åº¦é…ç½®
3. è‡ªåŠ¨åˆ›å»ºSGLangMixedPrecisionLoader
4. åŠ è½½æ··åˆç²¾åº¦æƒé‡åˆ°æ¨¡å‹
5. ä½¿ç”¨SGLangçš„æ¨ç†å¼•æ“è¿›è¡Œæ¨ç†
```

## ğŸ“Š ä¸“å®¶æ¿€æ´»è·Ÿè¸ªï¼ˆç‹¬ç«‹ç‰ˆæœ¬ï¼‰

### 2. APIæ¥å£
```python
# è·å–æ‰€æœ‰ä¸“å®¶ç»Ÿè®¡
stats = get_expert_activation_stats()

# è·å–ç‰¹å®šä¸“å®¶ç»Ÿè®¡
expert_stats = get_expert_activation_stats(layer_id=0, expert_id=1)

# è·å–çƒ­é—¨ä¸“å®¶
top_experts = get_expert_activation_stats()['top_experts']

# é‡ç½®ç»Ÿè®¡
reset_expert_activation_stats()

# å¯¼å‡ºç»Ÿè®¡
export_expert_activation_stats("expert_stats.json")
```

### 3. ç»Ÿè®¡æ•°æ®ç»“æ„
```json
{
  "expert_stats": {
    "layer_0_expert_1": {
      "layer_id": 0,
      "expert_id": 1,
      "activation_count": 150,
      "last_activation_time": 1640995200.0,
      "total_tokens_processed": 1500
    }
  },
  "layer_stats": {
    "0": {
      "total_experts": 8,
      "total_activations": 1200,
      "total_tokens": 12000,
      "experts": {
        "0": {"activation_count": 150, "total_tokens_processed": 1500},
        "1": {"activation_count": 120, "total_tokens_processed": 1200}
      }
    }
  },
  "top_experts": [
    {
      "layer_id": 0,
      "expert_id": 1,
      "activation_count": 150,
      "total_tokens_processed": 1500
    }
  ]
}
```

## ğŸ”§ GPTQæ”¯æŒ

### 1. GPTQæƒé‡æ ¼å¼
æ”¯æŒæ ‡å‡†çš„GPTQé‡åŒ–æ ¼å¼ï¼š
- `qweight`: é‡åŒ–çš„æƒé‡
- `qzeros`: é‡åŒ–çš„é›¶ç‚¹
- `scales`: ç¼©æ”¾å› å­
- `g_idx`: åˆ†ç»„ç´¢å¼•ï¼ˆå¯é€‰ï¼‰

### 2. è‡ªåŠ¨æ£€æµ‹
ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹GPTQæ ¼å¼å¹¶åº”ç”¨ç›¸åº”çš„åé‡åŒ–ç®—æ³•ï¼š
```python
# è‡ªåŠ¨æ£€æµ‹GPTQæ ¼å¼
if precision == 'int4' and is_gptq_weight(weights, weight_name):
    # ä½¿ç”¨GPTQåé‡åŒ–
    weight = dequantize_gptq_weight(qweight, qzeros, scales, g_idx)
else:
    # ä½¿ç”¨æ ‡å‡†åŠ è½½
    weight = weights[weight_name]
```

### 3. ä¿®å¤çš„GPTQåé‡åŒ–ç®—æ³•
æœ€æ–°ç‰ˆæœ¬ä¿®å¤äº†GPTQåé‡åŒ–ä¸­çš„ç»´åº¦åŒ¹é…é—®é¢˜ï¼š

#### é—®é¢˜æè¿°
```
ERROR: The size of tensor a (96) must match the size of tensor b (768) at non-singleton dimension 1
qweight: torch.Size([256, 768])
qzeros: torch.Size([16, 96])
scales: torch.Size([16, 768])
```

#### ä¿®å¤æ–¹æ¡ˆ
- **æ­£ç¡®çš„ç»´åº¦è®¡ç®—**: åŸºäºå®é™…çš„group_sizeè®¡ç®—æ‰©å±•å› å­
- **æ™ºèƒ½ç»´åº¦åŒ¹é…**: è‡ªåŠ¨è°ƒæ•´scaleså’Œzerosçš„ç»´åº¦ä»¥åŒ¹é…unpackedæƒé‡
- **è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯**: æä¾›å®Œæ•´çš„ç»´åº¦è®¡ç®—è¿‡ç¨‹æ—¥å¿—

#### ä¿®å¤æ–‡ä»¶
- `gptq_dequantizer_fixed.py`: ä¿®å¤çš„GPTQåé‡åŒ–å™¨
- `enhanced_mixed_precision_loader.py`: é›†æˆä¿®å¤çš„åŠ è½½å™¨
- `test_gptq_fix.py`: ä¿®å¤éªŒè¯æµ‹è¯•

#### æµ‹è¯•éªŒè¯
```bash
# è¿è¡ŒGPTQä¿®å¤æµ‹è¯•
python3 test_gptq_fix.py

# è¿è¡Œç®€å•æµ‹è¯•
python3 simple_gptq_test.py
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### 1. å†…å­˜ä¼˜åŒ–
- **æƒé‡ç¼“å­˜**: æ™ºèƒ½ç¼“å­˜å·²åŠ è½½çš„æƒé‡æ–‡ä»¶
- **é€‰æ‹©æ€§åŠ è½½**: åªåŠ è½½éœ€è¦çš„æƒé‡
- **å†…å­˜æ˜ å°„**: æ”¯æŒå¤§æ–‡ä»¶çš„æ‡’åŠ è½½

### 2. è®¡ç®—ä¼˜åŒ–
- **æ··åˆç²¾åº¦**: å¹³è¡¡ç²¾åº¦å’Œæ€§èƒ½
- **å‘é‡åŒ–æ“ä½œ**: ä¼˜åŒ–çš„GPTQåé‡åŒ–
- **å¹¶è¡Œå¤„ç†**: æ”¯æŒå¹¶å‘è¯·æ±‚

### 3. å­˜å‚¨ä¼˜åŒ–
- **å‹ç¼©å­˜å‚¨**: æ”¯æŒå¤šç§å‹ç¼©æ ¼å¼
- **ç´¢å¼•æ–‡ä»¶**: åˆ©ç”¨safetensorsç´¢å¼•
- **åˆ†ç‰‡åŠ è½½**: æ”¯æŒå¤§æ¨¡å‹çš„åˆ†ç‰‡åŠ è½½

## ğŸ”§ è®¾å¤‡é—®é¢˜ä¿®å¤

### 1. è®¾å¤‡ä¸åŒ¹é…é—®é¢˜
è‡ªåŠ¨æ£€æµ‹å’Œä¿®å¤CUDA/CPUè®¾å¤‡ä¸åŒ¹é…é—®é¢˜ï¼š

```python
from fix_device_issues import comprehensive_device_fix

# ç»¼åˆè®¾å¤‡ä¿®å¤
results = comprehensive_device_fix(model, tokenizer, 'cuda')
print(f"ä¿®å¤ç»“æœ: {results}")
```

### 2. æ³¨æ„åŠ›æ©ç ä¿®å¤
è‡ªåŠ¨å¤„ç†pad_tokenå’Œeos_tokenç›¸åŒçš„æƒ…å†µï¼š

```python
from fix_device_issues import create_proper_attention_mask

# åˆ›å»ºæ­£ç¡®çš„æ³¨æ„åŠ›æ©ç 
attention_mask = create_proper_attention_mask(input_ids, tokenizer, 'cuda')
```

### 3. MoEæ¨¡å—è®¾å¤‡ä¿®å¤
ä¸“é—¨å¤„ç†MoEæ¨¡å—çš„è®¾å¤‡é—®é¢˜ï¼š

```python
from fix_device_issues import fix_moe_device_issues

# ä¿®å¤MoEæ¨¡å—è®¾å¤‡é—®é¢˜
model = fix_moe_device_issues(model, 'cuda')
```

### 4. è®¾å¤‡ä¸€è‡´æ€§éªŒè¯
éªŒè¯æ¨¡å‹æ‰€æœ‰å‚æ•°éƒ½åœ¨æ­£ç¡®è®¾å¤‡ä¸Šï¼š

```python
from fix_device_issues import validate_model_device_consistency

# éªŒè¯è®¾å¤‡ä¸€è‡´æ€§
validation = validate_model_device_consistency(model, 'cuda')
if validation['is_consistent']:
    print("âœ“ è®¾å¤‡ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")
else:
    print(f"âš  å‘ç°è®¾å¤‡é—®é¢˜: {validation['issues']}")
```

## ğŸ§ª æµ‹è¯•å’ŒéªŒè¯

### 1. åŠŸèƒ½æµ‹è¯•
```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
python3 test_enhanced_features.py

# æµ‹è¯•ç‰¹å®šåŠŸèƒ½
python3 -c "
from sglang.srt.model_loader.enhanced_mixed_precision_loader import GPTQDequantizer
import torch
# æµ‹è¯•GPTQåé‡åŒ–
qweight = torch.randint(0, 16, (256, 768), dtype=torch.int32)
qzeros = torch.randint(0, 16, (16, 96), dtype=torch.int32)
scales = torch.randn(16, 768, dtype=torch.float16)
weight = GPTQDequantizer.dequantize_gptq_weight(qweight, qzeros, scales)
print(f'GPTQåé‡åŒ–æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {weight.shape}')
"
```

### 2. æ€§èƒ½æµ‹è¯•
```python
import time
from sglang.srt.enhanced_model_loader import load_model_with_enhanced_features

# æµ‹è¯•åŠ è½½æ—¶é—´
start_time = time.time()
stats = load_model_with_enhanced_features(model, config_path)
load_time = time.time() - start_time
print(f"æ¨¡å‹åŠ è½½æ—¶é—´: {load_time:.2f}ç§’")
```

## ğŸ” æ•…éšœæ’é™¤

### 1. å¸¸è§é—®é¢˜

**Q: é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯**
```bash
# æ£€æŸ¥YAMLæ ¼å¼
python3 -c "import yaml; yaml.safe_load(open('config.yaml'))"
```

**Q: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨**
```bash
# æ£€æŸ¥æ¨¡å‹è·¯å¾„
ls -la /path/to/model/
```

**Q: ä¸“å®¶è·Ÿè¸ªä¸å·¥ä½œ**
```python
# æ£€æŸ¥ä¸“å®¶è·Ÿè¸ªå™¨
from sglang.srt.model_loader.enhanced_mixed_precision_loader import get_global_expert_tracker
tracker = get_global_expert_tracker()
if tracker:
    print("ä¸“å®¶è·Ÿè¸ªå™¨å·²å¯ç”¨")
else:
    print("ä¸“å®¶è·Ÿè¸ªå™¨æœªå¯ç”¨")
```

**Q: è®¾å¤‡ä¸åŒ¹é…é”™è¯¯**
```
WARNING: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# è¿è¡Œè®¾å¤‡ä¿®å¤æµ‹è¯•
python3 test_device_fix.py

# ä½¿ç”¨è®¾å¤‡ä¿®å¤åŠŸèƒ½
from fix_device_issues import comprehensive_device_fix
results = comprehensive_device_fix(model, tokenizer, 'cuda')
```

**Q: æ³¨æ„åŠ›æ©ç è­¦å‘Š**
```
The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
```

**è§£å†³æ–¹æ¡ˆ**:
- ç³»ç»Ÿä¼šè‡ªåŠ¨åˆ›å»ºæ­£ç¡®çš„æ³¨æ„åŠ›æ©ç 
- ç¡®ä¿tokenizerçš„pad_token_idå’Œeos_token_idè®¾ç½®æ­£ç¡®

### 2. è°ƒè¯•æ¨¡å¼
```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
export PYTHONPATH=/path/to/sglang/python:$PYTHONPATH
python3 -u launch_enhanced_server.py --config config.yaml --model /path/to/model
```

## ğŸ“š APIå‚è€ƒ

### EnhancedModelLoader
```python
class EnhancedModelLoader:
    def __init__(self, config_path: str, enable_expert_tracking: bool = True)
    def load_model(self, model: torch.nn.Module, enable_moe_tracking: bool = True) -> Dict[str, Any]
    def get_expert_tracker(self) -> Optional[ExpertActivationTracker]
    def get_expert_stats(self, layer_id: Optional[int] = None, expert_id: Optional[int] = None) -> Dict
    def get_top_experts(self, top_k: int = 10) -> list
    def get_layer_stats(self) -> Dict
    def reset_expert_stats(self)
    def export_expert_stats(self, file_path: str)
```

### GPTQDequantizer
```python
class GPTQDequantizer:
    @staticmethod
    def dequantize_gptq_weight(qweight: torch.Tensor, qzeros: torch.Tensor, 
                              scales: torch.Tensor, g_idx: Optional[torch.Tensor] = None,
                              bits: int = 4, group_size: int = 128) -> torch.Tensor
```

### ExpertActivationTracker
```python
class ExpertActivationTracker:
    def record_expert_activation(self, layer_id: int, expert_id: int, 
                               tokens_processed: int = 1, request_id: str = None)
    def record_request(self, request_id: str, input_length: int, output_length: int)
    def get_expert_stats(self, layer_id: Optional[int] = None, expert_id: Optional[int] = None) -> Dict
    def get_top_experts(self, top_k: int = 10) -> List[Dict]
    def get_layer_stats(self) -> Dict[int, Dict]
    def reset_stats(self)
    def export_stats(self, file_path: str)
```

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›è¿™ä¸ªé¡¹ç›®ï¼

### å¼€å‘ç¯å¢ƒè®¾ç½®
```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd sglang-0.4.7

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# è¿è¡Œæµ‹è¯•
python3 test_enhanced_features.py
```

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäºSGLangçš„è®¸å¯è¯ï¼Œè¯·å‚è€ƒåŸå§‹é¡¹ç›®çš„è®¸å¯è¯æ–‡ä»¶ã€‚

## ğŸ™ è‡´è°¢

æ„Ÿè°¢SGLangå›¢é˜Ÿæä¾›çš„é«˜æ€§èƒ½LLMæ¨ç†æ¡†æ¶ï¼Œä»¥åŠå¼€æºç¤¾åŒºå¯¹GPTQå’ŒMoEæŠ€æœ¯çš„è´¡çŒ®ã€‚


# æ··åˆç²¾åº¦TP/DPä½¿ç”¨æŒ‡å—

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•ä½¿ç”¨å¼ é‡å¹¶è¡Œ(TP=4)å’Œæ•°æ®å¹¶è¡Œ(DP=2)çš„æ··åˆç²¾åº¦æ¨ç†ã€‚

## ğŸ¯ æ¦‚è¿°

- **å¼ é‡å¹¶è¡Œ(TP=4)**: å°†æ¨¡å‹æƒé‡åˆ†å‰²åˆ°4ä¸ªGPUä¸Šï¼Œå‡å°‘æ¯ä¸ªGPUçš„å†…å­˜ä½¿ç”¨
- **æ•°æ®å¹¶è¡Œ(DP=2)**: åœ¨2ä¸ªTPç»„ä¹‹é—´è¿›è¡Œæ•°æ®å¹¶è¡Œï¼Œæé«˜ååé‡
- **æ··åˆç²¾åº¦**: æ”¯æŒå¤šç§é‡åŒ–æ ¼å¼å…±å­˜ï¼Œä¿æŒå‹ç¼©æ ¼å¼ä»¥èŠ‚çœGPUå­˜å‚¨
- **æ€»è¿›ç¨‹æ•°**: 8ä¸ªè¿›ç¨‹ (TP=4 Ã— DP=2)

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

ç¡®ä¿æ‚¨çš„ç¯å¢ƒæ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š

```bash
# æ£€æŸ¥CUDAç¯å¢ƒ
nvidia-smi

# æ£€æŸ¥PyTorch
python3 -c "import torch; print(f'PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}')"

# æ£€æŸ¥GPUæ•°é‡ (è‡³å°‘éœ€è¦8ä¸ªGPUç”¨äºTP=4, DP=2)
nvidia-smi --list-gpus | wc -l
```

### 2. é…ç½®æ–‡ä»¶å‡†å¤‡

ç¡®ä¿æ‚¨æœ‰ä»¥ä¸‹æ–‡ä»¶ï¼š

```bash
# æ¨¡å‹è·¯å¾„
MODEL_PATH="/path/to/your/model"

# æ··åˆç²¾åº¦é…ç½®æ–‡ä»¶
MIXED_PRECISION_CONFIG="mixed_precision_config.yaml"

# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
ls -la $MODEL_PATH
ls -la $MIXED_PRECISION_CONFIG
```

### 3. å¯åŠ¨æ··åˆç²¾åº¦TP/DPæœåŠ¡å™¨

#### æ–¹æ³•1: ä½¿ç”¨å¯åŠ¨è„šæœ¬ (æ¨è)

```bash
# åŸºæœ¬å¯åŠ¨
./start_mixed_precision_tp_dp.sh \
  -m /path/to/your/model \
  -c mixed_precision_config.yaml

# è‡ªå®šä¹‰TP/DPé…ç½®
./start_mixed_precision_tp_dp.sh \
  -m /path/to/your/model \
  -c mixed_precision_config.yaml \
  -t 4 -d 2 \
  --test

# è‡ªå®šä¹‰åˆ†å¸ƒå¼åœ°å€
./start_mixed_precision_tp_dp.sh \
  -m /path/to/your/model \
  -c mixed_precision_config.yaml \
  -a "192.168.1.100:50000"
```

#### æ–¹æ³•2: æ‰‹åŠ¨å¯åŠ¨å•ä¸ªè¿›ç¨‹

```bash
# å¯åŠ¨8ä¸ªè¿›ç¨‹ (rank 0-7)
for rank in {0..7}; do
  python3 launch_mixed_precision_tp_dp.py \
    --model /path/to/your/model \
    --mixed-precision-config mixed_precision_config.yaml \
    --tp-size 4 \
    --dp-size 2 \
    --rank $rank \
    --world-size 8 \
    --dist-init-addr "127.0.0.1:50000" \
    --dtype auto &
done

# ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å¯åŠ¨
wait
```

## ğŸ“‹ é…ç½®è¯´æ˜

### æ··åˆç²¾åº¦é…ç½®æ–‡ä»¶

```yaml
# mixed_precision_config.yaml
mixed_precision:
  # ä¸åŒç²¾åº¦æƒé‡çš„è·¯å¾„
  fp16_path: "/path/to/fp16/weights"
  fp8_path: "/path/to/fp8/weights"
  gptq_int4_path: "/path/to/gptq_int4/weights"
  
  # æƒé‡æ˜ å°„é…ç½®
  weight_mapping:
    # æ³¨æ„åŠ›å±‚ä½¿ç”¨FP16ï¼ˆä¿æŒé«˜ç²¾åº¦ï¼‰
    "model.layers.0.self_attn.q_proj.weight": "fp16"
    "model.layers.0.self_attn.k_proj.weight": "fp16"
    "model.layers.0.self_attn.v_proj.weight": "fp16"
    "model.layers.0.self_attn.o_proj.weight": "fp16"
    
    # MLPå±‚ä½¿ç”¨FP8ï¼ˆä¸­ç­‰ç²¾åº¦ï¼‰
    "model.layers.0.mlp.gate_proj.weight": "fp8"
    "model.layers.0.mlp.up_proj.weight": "fp8"
    "model.layers.0.mlp.down_proj.weight": "fp8"
    
    # ä¸“å®¶å±‚ä½¿ç”¨GPTQ-Int4ï¼ˆé«˜å‹ç¼©æ¯”ï¼‰
    "model.layers.0.mlp.experts.0.gate_proj.weight": "gptq_int4"
    "model.layers.0.mlp.experts.0.up_proj.weight": "gptq_int4"
    "model.layers.0.mlp.experts.0.down_proj.weight": "gptq_int4"
```

### å¯åŠ¨å‚æ•°è¯´æ˜

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `-m, --model` | æ¨¡å‹è·¯å¾„ | å¿…éœ€ |
| `-c, --config` | æ··åˆç²¾åº¦é…ç½®æ–‡ä»¶ | å¿…éœ€ |
| `-t, --tp-size` | å¼ é‡å¹¶è¡Œå¤§å° | 4 |
| `-d, --dp-size` | æ•°æ®å¹¶è¡Œå¤§å° | 2 |
| `-a, --dist-addr` | åˆ†å¸ƒå¼åˆå§‹åŒ–åœ°å€ | 127.0.0.1:50000 |
| `--dtype` | æ•°æ®ç±»å‹ | auto |
| `--test` | è¿è¡Œæµ‹è¯•æ¨¡å¼ | false |

## ğŸ”§ æ¶æ„è¯´æ˜

### è¿›ç¨‹åˆ†å¸ƒ

```
TP=4, DP=2 é…ç½® (æ€»å…±8ä¸ªè¿›ç¨‹):

TP Group 0 (GPU 0-3): è¿›ç¨‹ rank 0,1,2,3
TP Group 1 (GPU 4-7): è¿›ç¨‹ rank 4,5,6,7

æ¯ä¸ªTPç»„å†…éƒ¨è¿›è¡Œå¼ é‡å¹¶è¡Œï¼Œä¸¤ä¸ªTPç»„ä¹‹é—´è¿›è¡Œæ•°æ®å¹¶è¡Œã€‚
```

### å†…å­˜ä½¿ç”¨

- **å¼ é‡å¹¶è¡Œ**: æ¯ä¸ªGPUåªåŠ è½½æ¨¡å‹æƒé‡çš„1/4
- **æ··åˆç²¾åº¦**: ä¸åŒå±‚ä½¿ç”¨ä¸åŒç²¾åº¦ï¼Œè¿›ä¸€æ­¥èŠ‚çœå†…å­˜
- **å‹ç¼©æ ¼å¼**: GPTQ-Int4ç­‰å‹ç¼©æ ¼å¼ä¿æŒå‹ç¼©çŠ¶æ€

### æ€§èƒ½ä¼˜åŠ¿

1. **å†…å­˜æ•ˆç‡**: å¼ é‡å¹¶è¡Œå‡å°‘æ¯ä¸ªGPUçš„å†…å­˜ä½¿ç”¨
2. **è®¡ç®—æ•ˆç‡**: æ•°æ®å¹¶è¡Œæé«˜ååé‡
3. **å­˜å‚¨æ•ˆç‡**: æ··åˆç²¾åº¦ä¿æŒå‹ç¼©æ ¼å¼ï¼ŒèŠ‚çœå­˜å‚¨ç©ºé—´

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é”™è¯¯

#### 1. "tensor model parallel group is not initialized"

**åŸå› **: åˆ†å¸ƒå¼ç¯å¢ƒæœªæ­£ç¡®åˆå§‹åŒ–

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ç¡®ä¿æ‰€æœ‰è¿›ç¨‹åŒæ—¶å¯åŠ¨
# æ£€æŸ¥åˆ†å¸ƒå¼åœ°å€æ˜¯å¦æ­£ç¡®
# ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸
```

#### 2. "pipeline model parallel group is not initialized"

**åŸå› **: æ¨¡å‹å¹¶è¡Œç»„æœªæ­£ç¡®åˆå§‹åŒ–

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥TP_SIZEå’ŒDP_SIZEçš„ä¹˜ç§¯æ˜¯å¦ç­‰äºWORLD_SIZE
# ç¡®ä¿æ‰€æœ‰è¿›ç¨‹ä½¿ç”¨ç›¸åŒçš„é…ç½®
```

#### 3. CUDAå†…å­˜ä¸è¶³

**åŸå› **: GPUå†…å­˜ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å‡å°‘TP_SIZEæˆ–DP_SIZE
# ä½¿ç”¨æ›´æ¿€è¿›çš„é‡åŒ–é…ç½®
# å¢åŠ GPUæ•°é‡
```

### è°ƒè¯•æŠ€å·§

```bash
# 1. æ£€æŸ¥è¿›ç¨‹çŠ¶æ€
ps aux | grep launch_mixed_precision_tp_dp

# 2. æ£€æŸ¥GPUä½¿ç”¨æƒ…å†µ
nvidia-smi

# 3. æ£€æŸ¥ç½‘ç»œè¿æ¥
netstat -an | grep 50000

# 4. æŸ¥çœ‹æ—¥å¿—
tail -f /tmp/mixed_precision_*.log
```

## ğŸ“Š æ€§èƒ½ç›‘æ§

### å†…å­˜ä½¿ç”¨ç›‘æ§

```python
# è·å–æ··åˆç²¾åº¦ç»Ÿè®¡
from sglang.srt.layers.mixed_precision_linear import get_mixed_precision_memory_stats

stats = get_mixed_precision_memory_stats()
print(f"å†…å­˜èŠ‚çœ: {stats['memory_saved_mb']:.2f}MB")
print(f"å‹ç¼©æ¯”: {stats['compression_ratio']:.2f}x")
```

### æ€§èƒ½æŒ‡æ ‡

- **å†…å­˜ä½¿ç”¨**: æ¯ä¸ªGPUçš„å†…å­˜ä½¿ç”¨é‡
- **å‹ç¼©æ¯”**: æ··åˆç²¾åº¦ç›¸å¯¹äºFP16çš„å‹ç¼©æ¯”
- **ååé‡**: æ¯ç§’å¤„ç†çš„tokenæ•°é‡
- **å»¶è¿Ÿ**: å•ä¸ªè¯·æ±‚çš„å¤„ç†æ—¶é—´

## ğŸ”„ æ‰©å±•é…ç½®

### å¤šèŠ‚ç‚¹éƒ¨ç½²

```bash
# èŠ‚ç‚¹0 (GPU 0-3)
./start_mixed_precision_tp_dp.sh \
  -m /path/to/model \
  -c mixed_precision_config.yaml \
  -a "node0:50000" \
  --rank 0

# èŠ‚ç‚¹1 (GPU 4-7)
./start_mixed_precision_tp_dp.sh \
  -m /path/to/model \
  -c mixed_precision_config.yaml \
  -a "node0:50000" \
  --rank 4
```

### è‡ªå®šä¹‰é‡åŒ–é…ç½®

```yaml
# æ›´æ¿€è¿›çš„é‡åŒ–é…ç½®
mixed_precision:
  weight_mapping:
    # æ›´å¤šå±‚ä½¿ç”¨Int4é‡åŒ–
    "model.layers.*.mlp.experts.*.gate_proj.weight": "gptq_int4"
    "model.layers.*.mlp.experts.*.up_proj.weight": "gptq_int4"
    "model.layers.*.mlp.experts.*.down_proj.weight": "gptq_int4"
```

## ğŸ“ æœ€ä½³å®è·µ

1. **GPUæ•°é‡**: ç¡®ä¿æœ‰è¶³å¤Ÿçš„GPUæ”¯æŒTPÃ—DPé…ç½®
2. **ç½‘ç»œå¸¦å®½**: å¤šèŠ‚ç‚¹éƒ¨ç½²éœ€è¦é«˜å¸¦å®½ç½‘ç»œ
3. **å†…å­˜ç®¡ç†**: ç›‘æ§GPUå†…å­˜ä½¿ç”¨ï¼Œé¿å…OOM
4. **è´Ÿè½½å‡è¡¡**: ç¡®ä¿è¯·æ±‚å‡åŒ€åˆ†å¸ƒåˆ°æ‰€æœ‰DPç»„
5. **ç›‘æ§å‘Šè­¦**: è®¾ç½®å†…å­˜å’Œæ€§èƒ½ç›‘æ§å‘Šè­¦

## ğŸ†˜ è·å–å¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·ï¼š

1. æ£€æŸ¥æœ¬æ–‡æ¡£çš„æ•…éšœæ’é™¤éƒ¨åˆ†
2. æŸ¥çœ‹å¯åŠ¨è„šæœ¬çš„æ—¥å¿—è¾“å‡º
3. ç¡®è®¤ç¯å¢ƒé…ç½®æ˜¯å¦æ­£ç¡®
4. è”ç³»æŠ€æœ¯æ”¯æŒå›¢é˜Ÿ

# SGLangåŸç”Ÿæ··åˆç²¾åº¦é›†æˆä½¿ç”¨æŒ‡å—

## æ¦‚è¿°

æœ¬é›†æˆæ–¹æ¡ˆå°†æ··åˆç²¾åº¦åŠŸèƒ½æ— ç¼é›†æˆåˆ°SGLangçš„åŸç”Ÿå¯åŠ¨æµç¨‹ä¸­ï¼Œå®Œå…¨å…¼å®¹SGLangçš„TP/DP/EPç­‰åˆ†å¸ƒå¼åŠŸèƒ½ï¼ŒåŒæ—¶ä¿æŒæœ€å°åŒ–ä¿®æ”¹åŸåˆ™ã€‚

## æ ¸å¿ƒç‰¹æ€§

### âœ… å®Œå…¨å…¼å®¹SGLangåŸç”ŸåŠŸèƒ½
- **å¼ é‡å¹¶è¡Œ (TP)**: æ”¯æŒ `--tp-size` å‚æ•°
- **æ•°æ®å¹¶è¡Œ (DP)**: æ”¯æŒ `--dp-size` å‚æ•°  
- **ä¸“å®¶å¹¶è¡Œ (EP)**: æ”¯æŒ `--ep-size` å‚æ•°
- **ä¸“å®¶MoE**: æ”¯æŒ `--enable-ep-moe` å‚æ•°
- **æ‰€æœ‰SGLangåŸç”Ÿå‚æ•°**: å®Œå…¨å…¼å®¹

### âœ… æ··åˆç²¾åº¦åŠŸèƒ½
- **é€‰æ‹©æ€§æƒé‡åŠ è½½**: æ ¹æ®é…ç½®æ–‡ä»¶é€‰æ‹©ä¸åŒç²¾åº¦çš„æƒé‡
- **å†…å­˜ä¼˜åŒ–**: ä¿æŒæƒé‡åœ¨å‹ç¼©æ ¼å¼ï¼ŒåŠ¨æ€åé‡åŒ–
- **æ€§èƒ½ä¼˜åŒ–**: åˆ©ç”¨SGLangçš„ä¼˜åŒ–åŠŸèƒ½åŠ é€Ÿæ¨ç†

### âœ… æœ€å°åŒ–ä¿®æ”¹
- ä»…åœ¨ `ServerArgs` ä¸­æ·»åŠ 2ä¸ªå‚æ•°
- åœ¨ `ModelConfig` ä¸­ä¼ é€’æ··åˆç²¾åº¦é…ç½®
- å¤ç”¨ç°æœ‰çš„æ¨¡å‹åŠ è½½æµç¨‹

## å¿«é€Ÿå¼€å§‹

### 1. æ ‡å‡†SGLangå¯åŠ¨ (æ— æ··åˆç²¾åº¦)

```bash
# ä½¿ç”¨åŸç”ŸSGLangå‘½ä»¤æ ¼å¼
./start_mixed_precision_sglang.sh \
  -m /path/to/your/model \
  -t 4 -d 2 \
  --dtype bfloat16 \
  --max-running-requests 32
```

### 2. å¯ç”¨æ··åˆç²¾åº¦çš„SGLangå¯åŠ¨

```bash
# å¯ç”¨æ··åˆç²¾åº¦åŠŸèƒ½
./start_mixed_precision_sglang.sh \
  -m /path/to/your/model \
  -c mixed_precision_config.yaml \
  --enable-mixed-precision \
  -t 4 -d 2 \
  --dtype bfloat16 \
  --max-running-requests 32
```

### 3. å…¼å®¹åŸç”ŸSGLangå‘½ä»¤æ ¼å¼

```bash
# å®Œå…¨å…¼å®¹åŸç”ŸSGLangå‘½ä»¤
./start_mixed_precision_sglang.sh \
  -m /path/to/your/model \
  --enable-mixed-precision \
  -c mixed_precision_config.yaml \
  --tp-size 4 --dp-size 2 --ep-size 1 \
  --max-running-requests 32 --max-total-tokens 40960 \
  --dtype bfloat16 --trust-remote-code \
  --attention-backend torch_native \
  --sampling-backend pytorch \
  --disable-cuda-graph \
  --disable-cuda-graph-padding \
  --kv-cache-dtype auto \
  --allow-auto-truncate \
  --chunked-prefill-size 16384
```

## é…ç½®æ–‡ä»¶æ ¼å¼

### æ··åˆç²¾åº¦é…ç½®æ–‡ä»¶ç¤ºä¾‹

```yaml
# mixed_precision_config.yaml
mixed_precision:
  # FP16æƒé‡æ–‡ä»¶è·¯å¾„
  fp16_path: "/path/to/fp16/model"
  
  # FP8æƒé‡æ–‡ä»¶è·¯å¾„  
  fp8_path: "/path/to/fp8/model"
  
  # Int4æƒé‡æ–‡ä»¶è·¯å¾„
  int4_path: "/path/to/int4/model"
  
  # æƒé‡æ˜ å°„é…ç½® - æŒ‡å®šå“ªäº›å±‚ä½¿ç”¨å“ªç§ç²¾åº¦
  weight_mapping:
    # æ³¨æ„åŠ›å±‚ä½¿ç”¨FP16
    "model.layers.0.self_attn.q_proj.weight": "fp16"
    "model.layers.0.self_attn.k_proj.weight": "fp16"
    "model.layers.0.self_attn.v_proj.weight": "fp16"
    "model.layers.0.self_attn.o_proj.weight": "fp16"
    
    # ä¸“å®¶å±‚ä½¿ç”¨Int4
    "model.layers.0.mlp.experts.0.gate_proj.weight": "int4"
    "model.layers.0.mlp.experts.0.up_proj.weight": "int4"
    "model.layers.0.mlp.experts.0.down_proj.weight": "int4"
    
    # å…¶ä»–å±‚ä½¿ç”¨FP16
    "model.layers.0.input_layernorm.weight": "fp16"
    "model.layers.0.post_attention_layernorm.weight": "fp16"
    "model.embed_tokens.weight": "fp16"
    "model.norm.weight": "fp16"
    "lm_head.weight": "fp16"
```

## å‚æ•°è¯´æ˜

### æ··åˆç²¾åº¦å‚æ•°

| å‚æ•° | ç±»å‹ | å¿…éœ€ | è¯´æ˜ |
|------|------|------|------|
| `--enable-mixed-precision` | flag | å¦ | å¯ç”¨æ··åˆç²¾åº¦åŠ è½½ |
| `--mixed-precision-config` | string | æ¡ä»¶ | æ··åˆç²¾åº¦é…ç½®æ–‡ä»¶è·¯å¾„ |

### SGLangåŸç”Ÿå‚æ•° (å®Œå…¨å…¼å®¹)

| å‚æ•° | è¯´æ˜ |
|------|------|
| `--model-path` | æ¨¡å‹è·¯å¾„ |
| `--tp-size` | å¼ é‡å¹¶è¡Œå¤§å° |
| `--dp-size` | æ•°æ®å¹¶è¡Œå¤§å° |
| `--ep-size` | ä¸“å®¶å¹¶è¡Œå¤§å° |
| `--enable-ep-moe` | å¯ç”¨ä¸“å®¶MoE |
| `--dtype` | æ•°æ®ç±»å‹ |
| `--max-running-requests` | æœ€å¤§è¿è¡Œè¯·æ±‚æ•° |
| `--max-total-tokens` | æœ€å¤§æ€»tokenæ•° |
| `--host` | æœåŠ¡å™¨ä¸»æœº |
| `--port` | æœåŠ¡å™¨ç«¯å£ |
| `--trust-remote-code` | ä¿¡ä»»è¿œç¨‹ä»£ç  |
| `--attention-backend` | æ³¨æ„åŠ›åç«¯ |
| `--sampling-backend` | é‡‡æ ·åç«¯ |

## æ¶æ„è®¾è®¡

### ä¿®æ”¹ç‚¹ (æœ€å°åŒ–)

1. **ServerArgsç±»** (2ä¸ªå‚æ•°)
   ```python
   # æ·»åŠ æ··åˆç²¾åº¦é…ç½®å‚æ•°
   mixed_precision_config: Optional[str] = None
   enable_mixed_precision: bool = False
   ```

2. **ModelConfigç±»** (1è¡Œä¿®æ”¹)
   ```python
   # ä¼ é€’æ··åˆç²¾åº¦é…ç½®
   mixed_precision_config=server_args.mixed_precision_config if server_args.enable_mixed_precision else None
   ```

3. **DefaultModelLoader** (å·²æœ‰é›†æˆ)
   - å¤ç”¨ç°æœ‰çš„æ··åˆç²¾åº¦åŠ è½½é€»è¾‘
   - è‡ªåŠ¨æ£€æµ‹å¹¶åº”ç”¨æ··åˆç²¾åº¦é…ç½®

### å·¥ä½œæµç¨‹

```
ç”¨æˆ·å‘½ä»¤ â†’ ServerArgs â†’ ModelConfig â†’ DefaultModelLoader â†’ æ··åˆç²¾åº¦åŠ è½½ â†’ SGLangæ¨ç†
```

## æ€§èƒ½ä¼˜åŠ¿

### å†…å­˜ä¼˜åŒ–
- **å‹ç¼©å­˜å‚¨**: æƒé‡ä¿æŒåœ¨å‹ç¼©æ ¼å¼ (Int4/FP8)
- **åŠ¨æ€åé‡åŒ–**: æ¨ç†æ—¶æŒ‰éœ€åé‡åŒ–
- **å†…å­˜èŠ‚çœ**: ç›¸æ¯”FP16èŠ‚çœ50-75%æ˜¾å­˜

### æ€§èƒ½ä¼˜åŒ–
- **SGLangä¼˜åŒ–**: åˆ©ç”¨SGLangçš„TP/DP/EPä¼˜åŒ–
- **æ··åˆç²¾åº¦**: å…³é”®å±‚ä½¿ç”¨é«˜ç²¾åº¦ï¼Œå…¶ä»–å±‚ä½¿ç”¨ä½ç²¾åº¦
- **ä¸“å®¶å¹¶è¡Œ**: æ”¯æŒMoEæ¨¡å‹çš„ä¸“å®¶å¹¶è¡Œ

## ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1: Qwen3-235B MoEæ¨¡å‹

```bash
# ä½¿ç”¨TP=4, DP=2, EP=1çš„é…ç½®
./start_mixed_precision_sglang.sh \
  -m /dcar-vepfs-trans-models/Qwen3-235B-A22B \
  -c qwen3_mixed_precision_config.yaml \
  --enable-mixed-precision \
  -t 4 -d 2 -e 1 \
  --enable-ep-moe \
  --max-running-requests 32 \
  --dtype bfloat16 \
  --trust-remote-code
```

### ç¤ºä¾‹2: æ ‡å‡†Transformeræ¨¡å‹

```bash
# ä½¿ç”¨TP=2, DP=2çš„é…ç½®
./start_mixed_precision_sglang.sh \
  -m /path/to/llama2-70b \
  -c llama2_mixed_precision_config.yaml \
  --enable-mixed-precision \
  -t 2 -d 2 \
  --max-running-requests 16 \
  --dtype bfloat16
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **é…ç½®æ–‡ä»¶ä¸å­˜åœ¨**
   ```
   é”™è¯¯: æ··åˆç²¾åº¦é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: config.yaml
   ```
   **è§£å†³**: ç¡®ä¿é…ç½®æ–‡ä»¶è·¯å¾„æ­£ç¡®

2. **GPUæ•°é‡ä¸è¶³**
   ```
   é”™è¯¯: GPUæ•°é‡ä¸è¶³
   éœ€è¦è‡³å°‘ 8 ä¸ªGPUï¼Œä½†åªæœ‰ 4 ä¸ªå¯ç”¨
   ```
   **è§£å†³**: è°ƒæ•´TP/DPé…ç½®æˆ–å¢åŠ GPU

3. **æ¨¡å‹è·¯å¾„é”™è¯¯**
   ```
   é”™è¯¯: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: /path/to/model
   ```
   **è§£å†³**: æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®

### è°ƒè¯•æŠ€å·§

1. **æŸ¥çœ‹è¯¦ç»†æ—¥å¿—**
   ```bash
   # å¯ç”¨è¯¦ç»†æ—¥å¿—
   export SGLANG_LOG_LEVEL=DEBUG
   ```

2. **æ£€æŸ¥GPUçŠ¶æ€**
   ```bash
   # ç›‘æ§GPUä½¿ç”¨æƒ…å†µ
   watch -n 1 nvidia-smi
   ```

3. **éªŒè¯é…ç½®**
   ```bash
   # æµ‹è¯•é…ç½®æ–‡ä»¶æ ¼å¼
   python3 -c "import yaml; yaml.safe_load(open('config.yaml'))"
   ```

## æ€»ç»“

æœ¬é›†æˆæ–¹æ¡ˆå®ç°äº†ï¼š

âœ… **å®Œå…¨å…¼å®¹**: ä¸SGLangåŸç”ŸåŠŸèƒ½100%å…¼å®¹  
âœ… **æœ€å°ä¿®æ”¹**: ä»…æ·»åŠ 2ä¸ªå‚æ•°ï¼Œå¤ç”¨ç°æœ‰ä»£ç   
âœ… **åŠŸèƒ½å®Œæ•´**: æ”¯æŒæ‰€æœ‰SGLangåŸç”ŸåŠŸèƒ½  
âœ… **æ€§èƒ½ä¼˜åŒ–**: å†…å­˜èŠ‚çœ50-75%ï¼Œä¿æŒæ¨ç†æ€§èƒ½  
âœ… **æ˜“äºä½¿ç”¨**: ç®€å•çš„å‘½ä»¤è¡Œå‚æ•°å³å¯å¯ç”¨  

ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨ç†Ÿæ‚‰çš„SGLangå‘½ä»¤æ ¼å¼æ¥å¯åŠ¨æ··åˆç²¾åº¦æ¨ç†äº†ï¼ğŸ‰
