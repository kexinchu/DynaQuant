# SGLangçœŸæ­£æ··åˆç²¾åº¦é›†æˆ

æœ¬é¡¹ç›®å°†**çœŸæ­£çš„æ··åˆç²¾åº¦**åŠŸèƒ½é›†æˆåˆ°SGLang 0.4.7çš„æ ¸å¿ƒæ¶æ„ä¸­ï¼Œæ”¯æŒå¤šç§é‡åŒ–æ ¼å¼å…±å­˜ï¼Œä¿æŒå„è‡ªçš„å‹ç¼©æ ¼å¼ä»¥èŠ‚çœGPUå­˜å‚¨ï¼Œè€Œä¸æ˜¯è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼ã€‚

## ğŸ¯ æ ¸å¿ƒä¼˜åŠ¿

### 1. **çœŸæ­£çš„æ··åˆç²¾åº¦**
- **å¤šç§é‡åŒ–æ ¼å¼å…±å­˜**: FP16ã€FP8ã€Int4ã€Int8æ ¼å¼åŒæ—¶å­˜åœ¨
- **ä¿æŒå‹ç¼©æ ¼å¼**: GPTQ Int4æƒé‡ä¿æŒå‹ç¼©æ ¼å¼ï¼ŒèŠ‚çœ75%å­˜å‚¨
- **åŠ¨æ€åé‡åŒ–**: ä»…åœ¨æ¨ç†æ—¶åé‡åŒ–ï¼Œä¸é¢„å…ˆè½¬æ¢
- **çœŸæ­£çš„å†…å­˜èŠ‚çœ**: ä¸æ˜¯è™šå‡çš„æ ¼å¼è½¬æ¢ï¼Œè€Œæ˜¯çœŸæ­£çš„å‹ç¼©å­˜å‚¨

### 2. **SGLangæ·±åº¦é›†æˆ**
- **ä½¿ç”¨SGLangçš„API**: é€šè¿‡`ModelConfig`ã€`DeviceConfig`ã€`LoadConfig`ç­‰SGLangæ ¸å¿ƒé…ç½®
- **åˆ©ç”¨SGLangä¼˜åŒ–**: ä½¿ç”¨SGLangçš„é«˜æ€§èƒ½æ¨ç†å¼•æ“å’Œå†…å­˜ç®¡ç†
- **ä¿æŒAPIå…¼å®¹**: å®Œå…¨å…¼å®¹SGLangçš„ç°æœ‰APIå’ŒåŠŸèƒ½

## ğŸš€ æ ¸å¿ƒåŠŸèƒ½

### 1. çœŸæ­£çš„æ··åˆç²¾åº¦æƒé‡åŠ è½½
- **å¤šç§é‡åŒ–æ ¼å¼å…±å­˜**: FP16ã€FP8ã€Int4ã€Int8æ ¼å¼åŒæ—¶å­˜åœ¨
- **ä¿æŒå‹ç¼©æ ¼å¼**: GPTQ Int4æƒé‡ä¿æŒå‹ç¼©æ ¼å¼ï¼ŒèŠ‚çœ75%å­˜å‚¨
- **åŠ¨æ€åé‡åŒ–**: ä»…åœ¨æ¨ç†æ—¶åé‡åŒ–ï¼Œä¸é¢„å…ˆè½¬æ¢
- **æƒé‡ç¼“å­˜**: æ”¯æŒåé‡åŒ–ç»“æœç¼“å­˜ï¼Œé¿å…é‡å¤è®¡ç®—

### 2. æ··åˆç²¾åº¦çº¿æ€§å±‚
- **åŠ¨æ€æ ¼å¼å¤„ç†**: æ ¹æ®æƒé‡æ ¼å¼åŠ¨æ€é€‰æ‹©å¤„ç†æ–¹å¼
- **ç¼“å­˜ä¼˜åŒ–**: æ”¯æŒæƒé‡ç¼“å­˜ï¼Œæé«˜æ¨ç†æ•ˆç‡
- **å†…å­˜ä¼˜åŒ–**: çœŸæ­£çš„å†…å­˜èŠ‚çœï¼Œä¸æ˜¯æ ¼å¼è½¬æ¢

### 3. ä¸“å®¶æ¿€æ´»è·Ÿè¸ªï¼ˆç‹¬ç«‹ç‰ˆæœ¬ï¼‰
- **å®æ—¶ç›‘æ§**: è·Ÿè¸ªMoEæ¨¡å‹ä¸­æ¯ä¸ªä¸“å®¶çš„æ¿€æ´»æƒ…å†µ
- **ç»Ÿè®¡åˆ†æ**: æä¾›è¯¦ç»†çš„ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡
- **æ€§èƒ½åˆ†æ**: åˆ†æä¸“å®¶åˆ©ç”¨ç‡å’Œè´Ÿè½½åˆ†å¸ƒ
- **æ•°æ®å¯¼å‡º**: æ”¯æŒç»Ÿè®¡æ•°æ®çš„å¯¼å‡ºå’Œåˆ†æ

## ğŸ“ æ–‡ä»¶ç»“æ„

```
sglang-0.4.7/
â”œâ”€â”€ python/sglang/srt/
â”‚   â”œâ”€â”€ model_loader/
â”‚   â”‚   â”œâ”€â”€ true_mixed_precision_loader.py      # çœŸæ­£çš„æ··åˆç²¾åº¦åŠ è½½å™¨
â”‚   â”‚   â”œâ”€â”€ sglang_mixed_precision_loader.py    # SGLangé›†æˆçš„æ··åˆç²¾åº¦åŠ è½½å™¨
â”‚   â”‚   â”œâ”€â”€ enhanced_mixed_precision_loader.py  # å¢å¼ºçš„æ··åˆç²¾åº¦åŠ è½½å™¨ï¼ˆç‹¬ç«‹ç‰ˆæœ¬ï¼‰
â”‚   â”‚   â””â”€â”€ loader.py                           # ä¿®æ”¹çš„SGLangåŠ è½½å™¨ï¼ˆé›†æˆæ··åˆç²¾åº¦ï¼‰
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â””â”€â”€ mixed_precision_linear.py           # æ··åˆç²¾åº¦çº¿æ€§å±‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ moe_tracker.py                      # MoEä¸“å®¶è·Ÿè¸ªå™¨
â”‚   â””â”€â”€ enhanced_model_loader.py                # å¢å¼ºçš„æ¨¡å‹åŠ è½½å™¨ï¼ˆç‹¬ç«‹ç‰ˆæœ¬ï¼‰
â”œâ”€â”€ launch_sglang_mixed_precision.py            # SGLangé›†æˆæœåŠ¡å™¨å¯åŠ¨è„šæœ¬
â”œâ”€â”€ test_true_mixed_precision.py                # çœŸæ­£æ··åˆç²¾åº¦åŠŸèƒ½æµ‹è¯•è„šæœ¬
â”œâ”€â”€ test_sglang_integration.py                  # SGLangé›†æˆæµ‹è¯•è„šæœ¬
â”œâ”€â”€ start_sglang_mixed_precision.sh             # SGLangé›†æˆå¯åŠ¨è„šæœ¬
â”œâ”€â”€ mixed_precision_config.yaml            # çœŸæ­£æ··åˆç²¾åº¦é…ç½®æ–‡ä»¶
â”œâ”€â”€ mixed_precision_config.yaml                 # æ··åˆç²¾åº¦é…ç½®æ–‡ä»¶
â””â”€â”€ README_ENHANCED_FEATURES.md                 # æœ¬æ–‡æ¡£
```

## ğŸ› ï¸ å®‰è£…å’Œé…ç½®

### 1. ç¯å¢ƒè¦æ±‚
```bash
# Python 3.8+
python3 --version

# å¿…è¦çš„åŒ…
pip install torch transformers safetensors pyyaml
```

### 2. é…ç½®æ–‡ä»¶
åˆ›å»ºæ··åˆç²¾åº¦é…ç½®æ–‡ä»¶ `mixed_precision_config.yaml`:

```yaml
mixed_precision:
  # ä¸åŒç²¾åº¦æƒé‡çš„è·¯å¾„
  fp16_path: "/path/to/fp16/weights"
  fp8_path: "/path/to/fp8/weights"
  int4_path: "/path/to/int4/weights"
  
  # æƒé‡æ˜ å°„é…ç½®
  weight_mapping:
    # æ³¨æ„åŠ›å±‚ä½¿ç”¨FP16
    "model.layers.0.self_attn.q_proj.weight": "fp16"
    "model.layers.0.self_attn.k_proj.weight": "fp16"
    "model.layers.0.self_attn.v_proj.weight": "fp16"
    "model.layers.0.self_attn.o_proj.weight": "fp16"
    
    # MLPå±‚ä½¿ç”¨FP8
    "model.layers.0.mlp.gate_proj.weight": "fp8"
    "model.layers.0.mlp.up_proj.weight": "fp8"
    "model.layers.0.mlp.down_proj.weight": "fp8"
    
    # ä¸“å®¶å±‚ä½¿ç”¨Int4
    "model.layers.0.mlp.experts.0.gate_proj.weight": "int4"
    "model.layers.0.mlp.experts.0.up_proj.weight": "int4"
    "model.layers.0.mlp.experts.0.down_proj.weight": "int4"

inference:
  max_seq_length: 4096
  max_batch_size: 32
  dtype: "bfloat16"
  device_map: "auto"

server:
  host: "127.0.0.1"
  port: 8080
  max_workers: 4
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. è¿è¡ŒçœŸæ­£æ··åˆç²¾åº¦æµ‹è¯•
```bash
# æµ‹è¯•çœŸæ­£æ··åˆç²¾åº¦åŠŸèƒ½
python3 test_true_mixed_precision.py

# æµ‹è¯•SGLangé›†æˆåŠŸèƒ½
python3 test_sglang_integration.py

# æˆ–è€…ä½¿ç”¨å¯åŠ¨è„šæœ¬æµ‹è¯•
./start_sglang_mixed_precision.sh --help
```

### 2. å¯åŠ¨çœŸæ­£æ··åˆç²¾åº¦æœåŠ¡å™¨
```bash
# ä½¿ç”¨çœŸæ­£æ··åˆç²¾åº¦é…ç½®
./start_sglang_mixed_precision.sh -m /path/to/model -c mixed_precision_config.yaml

# æˆ–è€…ç›´æ¥è¿è¡ŒPythonè„šæœ¬
python3 launch_sglang_mixed_precision.py \
  --model /path/to/model \
  --mixed-precision-config mixed_precision_config.yaml \
  --device cuda \
  --dtype auto \
  --test
```

### 3. ä½¿ç”¨çœŸæ­£æ··åˆç²¾åº¦API
```python
# ä½¿ç”¨çœŸæ­£æ··åˆç²¾åº¦åŠŸèƒ½
from sglang.srt.configs.model_config import ModelConfig
from sglang.srt.configs.device_config import DeviceConfig
from sglang.srt.configs.load_config import LoadConfig, LoadFormat
from sglang.srt.model_loader.loader import DefaultModelLoader

# åˆ›å»ºSGLangé…ç½®
model_config = ModelConfig(
    model_path="/path/to/model",
    mixed_precision_config="mixed_precision_config.yaml",
    dtype="auto",
    trust_remote_code=True
)

device_config = DeviceConfig(device="cuda")
load_config = LoadConfig(load_format=LoadFormat.AUTO)

# ä½¿ç”¨SGLangåŠ è½½å™¨åŠ è½½æ¨¡å‹ï¼ˆè‡ªåŠ¨ä½¿ç”¨çœŸæ­£æ··åˆç²¾åº¦ï¼‰
loader = DefaultModelLoader(load_config)
model = loader.load_model(
    model_config=model_config,
    device_config=device_config
)

# è·å–çœŸæ­£æ··åˆç²¾åº¦ç»Ÿè®¡
from sglang.srt.model_loader.true_mixed_precision_loader import get_global_true_mixed_precision_loader
from sglang.srt.layers.mixed_precision_linear import get_mixed_precision_memory_stats

mixed_precision_loader = get_global_true_mixed_precision_loader()
if mixed_precision_loader:
    memory_stats = get_mixed_precision_memory_stats()
    print(f"å†…å­˜èŠ‚çœ: {memory_stats['memory_saved_mb']:.2f}MB")
    print(f"å‹ç¼©æ¯”: {memory_stats['compression_ratio']:.2f}x")
```

## ğŸ”§ SGLangé›†æˆæ¶æ„

### 1. é›†æˆæ–¹å¼
- **ç»§æ‰¿SGLangåŸºç±»**: `SGLangMixedPrecisionLoader`ç»§æ‰¿è‡ª`ModelLoader`
- **ä½¿ç”¨SGLangé…ç½®**: é€šè¿‡`ModelConfig`çš„`mixed_precision_config`å‚æ•°
- **é›†æˆåˆ°åŠ è½½æµç¨‹**: åœ¨`DefaultModelLoader.load_model()`ä¸­è‡ªåŠ¨æ£€æµ‹å’Œä½¿ç”¨
- **ä¿æŒå‘åå…¼å®¹**: ä¸å½±å“SGLangçš„ç°æœ‰åŠŸèƒ½

### 2. æ ¸å¿ƒç»„ä»¶
- **SGLangMixedPrecisionLoader**: ç»§æ‰¿SGLangçš„ModelLoaderï¼Œæ”¯æŒæ··åˆç²¾åº¦
- **SGLangGPTQDequantizer**: é›†æˆåˆ°SGLangçš„GPTQåé‡åŒ–å™¨
- **MixedPrecisionConfig**: æ··åˆç²¾åº¦é…ç½®æ•°æ®ç»“æ„
- **å…¨å±€åŠ è½½å™¨ç®¡ç†**: é€šè¿‡å…¨å±€å˜é‡ç®¡ç†æ··åˆç²¾åº¦åŠ è½½å™¨å®ä¾‹

### 3. å·¥ä½œæµç¨‹
```
1. åˆ›å»ºModelConfigï¼ŒæŒ‡å®šmixed_precision_config
2. DefaultModelLoaderæ£€æµ‹åˆ°æ··åˆç²¾åº¦é…ç½®
3. è‡ªåŠ¨åˆ›å»ºSGLangMixedPrecisionLoader
4. åŠ è½½æ··åˆç²¾åº¦æƒé‡åˆ°æ¨¡å‹
5. ä½¿ç”¨SGLangçš„æ¨ç†å¼•æ“è¿›è¡Œæ¨ç†
```

## ğŸ“Š ä¸“å®¶æ¿€æ´»è·Ÿè¸ªï¼ˆç‹¬ç«‹ç‰ˆæœ¬ï¼‰

### 2. APIæ¥å£
```python
# è·å–æ‰€æœ‰ä¸“å®¶ç»Ÿè®¡
stats = get_expert_activation_stats()

# è·å–ç‰¹å®šä¸“å®¶ç»Ÿè®¡
expert_stats = get_expert_activation_stats(layer_id=0, expert_id=1)

# è·å–çƒ­é—¨ä¸“å®¶
top_experts = get_expert_activation_stats()['top_experts']

# é‡ç½®ç»Ÿè®¡
reset_expert_activation_stats()

# å¯¼å‡ºç»Ÿè®¡
export_expert_activation_stats("expert_stats.json")
```

### 3. ç»Ÿè®¡æ•°æ®ç»“æ„
```json
{
  "expert_stats": {
    "layer_0_expert_1": {
      "layer_id": 0,
      "expert_id": 1,
      "activation_count": 150,
      "last_activation_time": 1640995200.0,
      "total_tokens_processed": 1500
    }
  },
  "layer_stats": {
    "0": {
      "total_experts": 8,
      "total_activations": 1200,
      "total_tokens": 12000,
      "experts": {
        "0": {"activation_count": 150, "total_tokens_processed": 1500},
        "1": {"activation_count": 120, "total_tokens_processed": 1200}
      }
    }
  },
  "top_experts": [
    {
      "layer_id": 0,
      "expert_id": 1,
      "activation_count": 150,
      "total_tokens_processed": 1500
    }
  ]
}
```

## ğŸ”§ GPTQæ”¯æŒ

### 1. GPTQæƒé‡æ ¼å¼
æ”¯æŒæ ‡å‡†çš„GPTQé‡åŒ–æ ¼å¼ï¼š
- `qweight`: é‡åŒ–çš„æƒé‡
- `qzeros`: é‡åŒ–çš„é›¶ç‚¹
- `scales`: ç¼©æ”¾å› å­
- `g_idx`: åˆ†ç»„ç´¢å¼•ï¼ˆå¯é€‰ï¼‰

### 2. è‡ªåŠ¨æ£€æµ‹
ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹GPTQæ ¼å¼å¹¶åº”ç”¨ç›¸åº”çš„åé‡åŒ–ç®—æ³•ï¼š
```python
# è‡ªåŠ¨æ£€æµ‹GPTQæ ¼å¼
if precision == 'int4' and is_gptq_weight(weights, weight_name):
    # ä½¿ç”¨GPTQåé‡åŒ–
    weight = dequantize_gptq_weight(qweight, qzeros, scales, g_idx)
else:
    # ä½¿ç”¨æ ‡å‡†åŠ è½½
    weight = weights[weight_name]
```

### 3. ä¿®å¤çš„GPTQåé‡åŒ–ç®—æ³•
æœ€æ–°ç‰ˆæœ¬ä¿®å¤äº†GPTQåé‡åŒ–ä¸­çš„ç»´åº¦åŒ¹é…é—®é¢˜ï¼š

#### é—®é¢˜æè¿°
```
ERROR: The size of tensor a (96) must match the size of tensor b (768) at non-singleton dimension 1
qweight: torch.Size([256, 768])
qzeros: torch.Size([16, 96])
scales: torch.Size([16, 768])
```

#### ä¿®å¤æ–¹æ¡ˆ
- **æ­£ç¡®çš„ç»´åº¦è®¡ç®—**: åŸºäºå®é™…çš„group_sizeè®¡ç®—æ‰©å±•å› å­
- **æ™ºèƒ½ç»´åº¦åŒ¹é…**: è‡ªåŠ¨è°ƒæ•´scaleså’Œzerosçš„ç»´åº¦ä»¥åŒ¹é…unpackedæƒé‡
- **è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯**: æä¾›å®Œæ•´çš„ç»´åº¦è®¡ç®—è¿‡ç¨‹æ—¥å¿—

#### ä¿®å¤æ–‡ä»¶
- `gptq_dequantizer_fixed.py`: ä¿®å¤çš„GPTQåé‡åŒ–å™¨
- `enhanced_mixed_precision_loader.py`: é›†æˆä¿®å¤çš„åŠ è½½å™¨
- `test_gptq_fix.py`: ä¿®å¤éªŒè¯æµ‹è¯•

#### æµ‹è¯•éªŒè¯
```bash
# è¿è¡ŒGPTQä¿®å¤æµ‹è¯•
python3 test_gptq_fix.py

# è¿è¡Œç®€å•æµ‹è¯•
python3 simple_gptq_test.py
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### 1. å†…å­˜ä¼˜åŒ–
- **æƒé‡ç¼“å­˜**: æ™ºèƒ½ç¼“å­˜å·²åŠ è½½çš„æƒé‡æ–‡ä»¶
- **é€‰æ‹©æ€§åŠ è½½**: åªåŠ è½½éœ€è¦çš„æƒé‡
- **å†…å­˜æ˜ å°„**: æ”¯æŒå¤§æ–‡ä»¶çš„æ‡’åŠ è½½

### 2. è®¡ç®—ä¼˜åŒ–
- **æ··åˆç²¾åº¦**: å¹³è¡¡ç²¾åº¦å’Œæ€§èƒ½
- **å‘é‡åŒ–æ“ä½œ**: ä¼˜åŒ–çš„GPTQåé‡åŒ–
- **å¹¶è¡Œå¤„ç†**: æ”¯æŒå¹¶å‘è¯·æ±‚

### 3. å­˜å‚¨ä¼˜åŒ–
- **å‹ç¼©å­˜å‚¨**: æ”¯æŒå¤šç§å‹ç¼©æ ¼å¼
- **ç´¢å¼•æ–‡ä»¶**: åˆ©ç”¨safetensorsç´¢å¼•
- **åˆ†ç‰‡åŠ è½½**: æ”¯æŒå¤§æ¨¡å‹çš„åˆ†ç‰‡åŠ è½½

## ğŸ”§ è®¾å¤‡é—®é¢˜ä¿®å¤

### 1. è®¾å¤‡ä¸åŒ¹é…é—®é¢˜
è‡ªåŠ¨æ£€æµ‹å’Œä¿®å¤CUDA/CPUè®¾å¤‡ä¸åŒ¹é…é—®é¢˜ï¼š

```python
from fix_device_issues import comprehensive_device_fix

# ç»¼åˆè®¾å¤‡ä¿®å¤
results = comprehensive_device_fix(model, tokenizer, 'cuda')
print(f"ä¿®å¤ç»“æœ: {results}")
```

### 2. æ³¨æ„åŠ›æ©ç ä¿®å¤
è‡ªåŠ¨å¤„ç†pad_tokenå’Œeos_tokenç›¸åŒçš„æƒ…å†µï¼š

```python
from fix_device_issues import create_proper_attention_mask

# åˆ›å»ºæ­£ç¡®çš„æ³¨æ„åŠ›æ©ç 
attention_mask = create_proper_attention_mask(input_ids, tokenizer, 'cuda')
```

### 3. MoEæ¨¡å—è®¾å¤‡ä¿®å¤
ä¸“é—¨å¤„ç†MoEæ¨¡å—çš„è®¾å¤‡é—®é¢˜ï¼š

```python
from fix_device_issues import fix_moe_device_issues

# ä¿®å¤MoEæ¨¡å—è®¾å¤‡é—®é¢˜
model = fix_moe_device_issues(model, 'cuda')
```

### 4. è®¾å¤‡ä¸€è‡´æ€§éªŒè¯
éªŒè¯æ¨¡å‹æ‰€æœ‰å‚æ•°éƒ½åœ¨æ­£ç¡®è®¾å¤‡ä¸Šï¼š

```python
from fix_device_issues import validate_model_device_consistency

# éªŒè¯è®¾å¤‡ä¸€è‡´æ€§
validation = validate_model_device_consistency(model, 'cuda')
if validation['is_consistent']:
    print("âœ“ è®¾å¤‡ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")
else:
    print(f"âš  å‘ç°è®¾å¤‡é—®é¢˜: {validation['issues']}")
```

## ğŸ§ª æµ‹è¯•å’ŒéªŒè¯

### 1. åŠŸèƒ½æµ‹è¯•
```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
python3 test_enhanced_features.py

# æµ‹è¯•ç‰¹å®šåŠŸèƒ½
python3 -c "
from sglang.srt.model_loader.enhanced_mixed_precision_loader import GPTQDequantizer
import torch
# æµ‹è¯•GPTQåé‡åŒ–
qweight = torch.randint(0, 16, (256, 768), dtype=torch.int32)
qzeros = torch.randint(0, 16, (16, 96), dtype=torch.int32)
scales = torch.randn(16, 768, dtype=torch.float16)
weight = GPTQDequantizer.dequantize_gptq_weight(qweight, qzeros, scales)
print(f'GPTQåé‡åŒ–æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {weight.shape}')
"
```

### 2. æ€§èƒ½æµ‹è¯•
```python
import time
from sglang.srt.enhanced_model_loader import load_model_with_enhanced_features

# æµ‹è¯•åŠ è½½æ—¶é—´
start_time = time.time()
stats = load_model_with_enhanced_features(model, config_path)
load_time = time.time() - start_time
print(f"æ¨¡å‹åŠ è½½æ—¶é—´: {load_time:.2f}ç§’")
```

## ğŸ” æ•…éšœæ’é™¤

### 1. å¸¸è§é—®é¢˜

**Q: é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯**
```bash
# æ£€æŸ¥YAMLæ ¼å¼
python3 -c "import yaml; yaml.safe_load(open('config.yaml'))"
```

**Q: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨**
```bash
# æ£€æŸ¥æ¨¡å‹è·¯å¾„
ls -la /path/to/model/
```

**Q: ä¸“å®¶è·Ÿè¸ªä¸å·¥ä½œ**
```python
# æ£€æŸ¥ä¸“å®¶è·Ÿè¸ªå™¨
from sglang.srt.model_loader.enhanced_mixed_precision_loader import get_global_expert_tracker
tracker = get_global_expert_tracker()
if tracker:
    print("ä¸“å®¶è·Ÿè¸ªå™¨å·²å¯ç”¨")
else:
    print("ä¸“å®¶è·Ÿè¸ªå™¨æœªå¯ç”¨")
```

**Q: è®¾å¤‡ä¸åŒ¹é…é”™è¯¯**
```
WARNING: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# è¿è¡Œè®¾å¤‡ä¿®å¤æµ‹è¯•
python3 test_device_fix.py

# ä½¿ç”¨è®¾å¤‡ä¿®å¤åŠŸèƒ½
from fix_device_issues import comprehensive_device_fix
results = comprehensive_device_fix(model, tokenizer, 'cuda')
```

**Q: æ³¨æ„åŠ›æ©ç è­¦å‘Š**
```
The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
```

**è§£å†³æ–¹æ¡ˆ**:
- ç³»ç»Ÿä¼šè‡ªåŠ¨åˆ›å»ºæ­£ç¡®çš„æ³¨æ„åŠ›æ©ç 
- ç¡®ä¿tokenizerçš„pad_token_idå’Œeos_token_idè®¾ç½®æ­£ç¡®

### 2. è°ƒè¯•æ¨¡å¼
```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
export PYTHONPATH=/path/to/sglang/python:$PYTHONPATH
python3 -u launch_enhanced_server.py --config config.yaml --model /path/to/model
```

## ğŸ“š APIå‚è€ƒ

### EnhancedModelLoader
```python
class EnhancedModelLoader:
    def __init__(self, config_path: str, enable_expert_tracking: bool = True)
    def load_model(self, model: torch.nn.Module, enable_moe_tracking: bool = True) -> Dict[str, Any]
    def get_expert_tracker(self) -> Optional[ExpertActivationTracker]
    def get_expert_stats(self, layer_id: Optional[int] = None, expert_id: Optional[int] = None) -> Dict
    def get_top_experts(self, top_k: int = 10) -> list
    def get_layer_stats(self) -> Dict
    def reset_expert_stats(self)
    def export_expert_stats(self, file_path: str)
```

### GPTQDequantizer
```python
class GPTQDequantizer:
    @staticmethod
    def dequantize_gptq_weight(qweight: torch.Tensor, qzeros: torch.Tensor, 
                              scales: torch.Tensor, g_idx: Optional[torch.Tensor] = None,
                              bits: int = 4, group_size: int = 128) -> torch.Tensor
```

### ExpertActivationTracker
```python
class ExpertActivationTracker:
    def record_expert_activation(self, layer_id: int, expert_id: int, 
                               tokens_processed: int = 1, request_id: str = None)
    def record_request(self, request_id: str, input_length: int, output_length: int)
    def get_expert_stats(self, layer_id: Optional[int] = None, expert_id: Optional[int] = None) -> Dict
    def get_top_experts(self, top_k: int = 10) -> List[Dict]
    def get_layer_stats(self) -> Dict[int, Dict]
    def reset_stats(self)
    def export_stats(self, file_path: str)
```

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›è¿™ä¸ªé¡¹ç›®ï¼

### å¼€å‘ç¯å¢ƒè®¾ç½®
```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd sglang-0.4.7

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# è¿è¡Œæµ‹è¯•
python3 test_enhanced_features.py
```

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäºSGLangçš„è®¸å¯è¯ï¼Œè¯·å‚è€ƒåŸå§‹é¡¹ç›®çš„è®¸å¯è¯æ–‡ä»¶ã€‚

## ğŸ™ è‡´è°¢

æ„Ÿè°¢SGLangå›¢é˜Ÿæä¾›çš„é«˜æ€§èƒ½LLMæ¨ç†æ¡†æ¶ï¼Œä»¥åŠå¼€æºç¤¾åŒºå¯¹GPTQå’ŒMoEæŠ€æœ¯çš„è´¡çŒ®ã€‚
