# SGLangå¢å¼ºåŠŸèƒ½é›†æˆ

æœ¬é¡¹ç›®å°†æ··åˆç²¾åº¦æƒé‡åŠ è½½å’Œä¸“å®¶æ¿€æ´»è·Ÿè¸ªåŠŸèƒ½é›†æˆåˆ°SGLang 0.4.7ä¸­ï¼Œæä¾›é«˜æ•ˆçš„æ¨¡å‹æ¨ç†å’Œä¸“å®¶è¡Œä¸ºåˆ†æèƒ½åŠ›ã€‚

## ğŸš€ æ ¸å¿ƒåŠŸèƒ½

### 1. æ··åˆç²¾åº¦æƒé‡åŠ è½½
- **é€‰æ‹©æ€§æƒé‡åŠ è½½**: æ ¹æ®é…ç½®æ–‡ä»¶é€‰æ‹©æ€§åŠ è½½ä¸åŒç²¾åº¦çš„æƒé‡
- **GPTQæ”¯æŒ**: å®Œæ•´çš„GPTQ-Int4é‡åŒ–æ¨¡å‹æ”¯æŒ
- **Safetensorså…¼å®¹**: æ”¯æŒsafetensorsç´¢å¼•æ–‡ä»¶
- **å†…å­˜ä¼˜åŒ–**: æ™ºèƒ½ç¼“å­˜å’Œå†…å­˜ç®¡ç†

### 2. ä¸“å®¶æ¿€æ´»è·Ÿè¸ª
- **å®æ—¶ç›‘æ§**: è·Ÿè¸ªMoEæ¨¡å‹ä¸­æ¯ä¸ªä¸“å®¶çš„æ¿€æ´»æƒ…å†µ
- **ç»Ÿè®¡åˆ†æ**: æä¾›è¯¦ç»†çš„ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡
- **æ€§èƒ½åˆ†æ**: åˆ†æä¸“å®¶åˆ©ç”¨ç‡å’Œè´Ÿè½½åˆ†å¸ƒ
- **æ•°æ®å¯¼å‡º**: æ”¯æŒç»Ÿè®¡æ•°æ®çš„å¯¼å‡ºå’Œåˆ†æ

## ğŸ“ æ–‡ä»¶ç»“æ„

```
sglang-0.4.7/
â”œâ”€â”€ python/sglang/srt/
â”‚   â”œâ”€â”€ model_loader/
â”‚   â”‚   â”œâ”€â”€ enhanced_mixed_precision_loader.py  # å¢å¼ºçš„æ··åˆç²¾åº¦åŠ è½½å™¨
â”‚   â”‚   â””â”€â”€ mixed_precision_loader.py           # åŸå§‹æ··åˆç²¾åº¦åŠ è½½å™¨
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ moe_tracker.py                      # MoEä¸“å®¶è·Ÿè¸ªå™¨
â”‚   â””â”€â”€ enhanced_model_loader.py                # å¢å¼ºçš„æ¨¡å‹åŠ è½½å™¨
â”œâ”€â”€ launch_enhanced_server.py                   # å¢å¼ºæœåŠ¡å™¨å¯åŠ¨è„šæœ¬
â”œâ”€â”€ test_enhanced_features.py                   # åŠŸèƒ½æµ‹è¯•è„šæœ¬
â”œâ”€â”€ start_enhanced_server.sh                    # å¯åŠ¨è„šæœ¬
â”œâ”€â”€ mixed_precision_config.yaml                 # æ··åˆç²¾åº¦é…ç½®æ–‡ä»¶
â””â”€â”€ README_ENHANCED_FEATURES.md                 # æœ¬æ–‡æ¡£
```

## ğŸ› ï¸ å®‰è£…å’Œé…ç½®

### 1. ç¯å¢ƒè¦æ±‚
```bash
# Python 3.8+
python3 --version

# å¿…è¦çš„åŒ…
pip install torch transformers safetensors pyyaml
```

### 2. é…ç½®æ–‡ä»¶
åˆ›å»ºæ··åˆç²¾åº¦é…ç½®æ–‡ä»¶ `mixed_precision_config.yaml`:

```yaml
mixed_precision:
  # ä¸åŒç²¾åº¦æƒé‡çš„è·¯å¾„
  fp16_path: "/path/to/fp16/weights"
  fp8_path: "/path/to/fp8/weights"
  int4_path: "/path/to/int4/weights"
  
  # æƒé‡æ˜ å°„é…ç½®
  weight_mapping:
    # æ³¨æ„åŠ›å±‚ä½¿ç”¨FP16
    "model.layers.0.self_attn.q_proj.weight": "fp16"
    "model.layers.0.self_attn.k_proj.weight": "fp16"
    "model.layers.0.self_attn.v_proj.weight": "fp16"
    "model.layers.0.self_attn.o_proj.weight": "fp16"
    
    # MLPå±‚ä½¿ç”¨FP8
    "model.layers.0.mlp.gate_proj.weight": "fp8"
    "model.layers.0.mlp.up_proj.weight": "fp8"
    "model.layers.0.mlp.down_proj.weight": "fp8"
    
    # ä¸“å®¶å±‚ä½¿ç”¨Int4
    "model.layers.0.mlp.experts.0.gate_proj.weight": "int4"
    "model.layers.0.mlp.experts.0.up_proj.weight": "int4"
    "model.layers.0.mlp.experts.0.down_proj.weight": "int4"

inference:
  max_seq_length: 4096
  max_batch_size: 32
  dtype: "bfloat16"
  device_map: "auto"

server:
  host: "127.0.0.1"
  port: 8080
  max_workers: 4
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. è¿è¡ŒåŠŸèƒ½æµ‹è¯•
```bash
# æµ‹è¯•æ‰€æœ‰å¢å¼ºåŠŸèƒ½
./start_enhanced_server.sh --test

# æˆ–è€…ç›´æ¥è¿è¡Œæµ‹è¯•è„šæœ¬
python3 test_enhanced_features.py
```

### 2. å¯åŠ¨å¢å¼ºæœåŠ¡å™¨
```bash
# ä½¿ç”¨å¯åŠ¨è„šæœ¬
./start_enhanced_server.sh -m /path/to/model -c mixed_precision_config.yaml

# æˆ–è€…ç›´æ¥è¿è¡ŒPythonè„šæœ¬
python3 launch_enhanced_server.py \
  --config mixed_precision_config.yaml \
  --model /path/to/model \
  --port 8080 \
  --enable-expert-tracking
```

### 3. ä½¿ç”¨API
```python
from sglang.srt.enhanced_model_loader import (
    load_model_with_enhanced_features,
    get_expert_activation_stats,
    reset_expert_activation_stats
)

# åŠ è½½æ¨¡å‹
stats = load_model_with_enhanced_features(
    model, config_path, enable_expert_tracking=True
)

# è·å–ä¸“å®¶ç»Ÿè®¡
expert_stats = get_expert_activation_stats()
print(f"ä¸“å®¶ç»Ÿè®¡: {expert_stats}")
```

## ğŸ“Š ä¸“å®¶æ¿€æ´»è·Ÿè¸ª

### 1. ç»Ÿè®¡ä¿¡æ¯ç±»å‹
- **ä¸“å®¶æ¿€æ´»æ¬¡æ•°**: æ¯ä¸ªä¸“å®¶è¢«æ¿€æ´»çš„æ€»æ¬¡æ•°
- **æ¿€æ´»æ—¶é—´**: æœ€åä¸€æ¬¡æ¿€æ´»çš„æ—¶é—´æˆ³
- **å¤„ç†tokenæ•°**: æ¯ä¸ªä¸“å®¶å¤„ç†çš„tokenæ€»æ•°
- **å±‚ç»Ÿè®¡**: æ¯å±‚çš„ä¸“å®¶ä½¿ç”¨æƒ…å†µ
- **çƒ­é—¨ä¸“å®¶**: æ¿€æ´»æ¬¡æ•°æœ€å¤šçš„ä¸“å®¶æ’å

### 2. APIæ¥å£
```python
# è·å–æ‰€æœ‰ä¸“å®¶ç»Ÿè®¡
stats = get_expert_activation_stats()

# è·å–ç‰¹å®šä¸“å®¶ç»Ÿè®¡
expert_stats = get_expert_activation_stats(layer_id=0, expert_id=1)

# è·å–çƒ­é—¨ä¸“å®¶
top_experts = get_expert_activation_stats()['top_experts']

# é‡ç½®ç»Ÿè®¡
reset_expert_activation_stats()

# å¯¼å‡ºç»Ÿè®¡
export_expert_activation_stats("expert_stats.json")
```

### 3. ç»Ÿè®¡æ•°æ®ç»“æ„
```json
{
  "expert_stats": {
    "layer_0_expert_1": {
      "layer_id": 0,
      "expert_id": 1,
      "activation_count": 150,
      "last_activation_time": 1640995200.0,
      "total_tokens_processed": 1500
    }
  },
  "layer_stats": {
    "0": {
      "total_experts": 8,
      "total_activations": 1200,
      "total_tokens": 12000,
      "experts": {
        "0": {"activation_count": 150, "total_tokens_processed": 1500},
        "1": {"activation_count": 120, "total_tokens_processed": 1200}
      }
    }
  },
  "top_experts": [
    {
      "layer_id": 0,
      "expert_id": 1,
      "activation_count": 150,
      "total_tokens_processed": 1500
    }
  ]
}
```

## ğŸ”§ GPTQæ”¯æŒ

### 1. GPTQæƒé‡æ ¼å¼
æ”¯æŒæ ‡å‡†çš„GPTQé‡åŒ–æ ¼å¼ï¼š
- `qweight`: é‡åŒ–çš„æƒé‡
- `qzeros`: é‡åŒ–çš„é›¶ç‚¹
- `scales`: ç¼©æ”¾å› å­
- `g_idx`: åˆ†ç»„ç´¢å¼•ï¼ˆå¯é€‰ï¼‰

### 2. è‡ªåŠ¨æ£€æµ‹
ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹GPTQæ ¼å¼å¹¶åº”ç”¨ç›¸åº”çš„åé‡åŒ–ç®—æ³•ï¼š
```python
# è‡ªåŠ¨æ£€æµ‹GPTQæ ¼å¼
if precision == 'int4' and is_gptq_weight(weights, weight_name):
    # ä½¿ç”¨GPTQåé‡åŒ–
    weight = dequantize_gptq_weight(qweight, qzeros, scales, g_idx)
else:
    # ä½¿ç”¨æ ‡å‡†åŠ è½½
    weight = weights[weight_name]
```

### 3. ä¿®å¤çš„GPTQåé‡åŒ–ç®—æ³•
æœ€æ–°ç‰ˆæœ¬ä¿®å¤äº†GPTQåé‡åŒ–ä¸­çš„ç»´åº¦åŒ¹é…é—®é¢˜ï¼š

#### é—®é¢˜æè¿°
```
ERROR: The size of tensor a (96) must match the size of tensor b (768) at non-singleton dimension 1
qweight: torch.Size([256, 768])
qzeros: torch.Size([16, 96])
scales: torch.Size([16, 768])
```

#### ä¿®å¤æ–¹æ¡ˆ
- **æ­£ç¡®çš„ç»´åº¦è®¡ç®—**: åŸºäºå®é™…çš„group_sizeè®¡ç®—æ‰©å±•å› å­
- **æ™ºèƒ½ç»´åº¦åŒ¹é…**: è‡ªåŠ¨è°ƒæ•´scaleså’Œzerosçš„ç»´åº¦ä»¥åŒ¹é…unpackedæƒé‡
- **è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯**: æä¾›å®Œæ•´çš„ç»´åº¦è®¡ç®—è¿‡ç¨‹æ—¥å¿—

#### ä¿®å¤æ–‡ä»¶
- `gptq_dequantizer_fixed.py`: ä¿®å¤çš„GPTQåé‡åŒ–å™¨
- `enhanced_mixed_precision_loader.py`: é›†æˆä¿®å¤çš„åŠ è½½å™¨
- `test_gptq_fix.py`: ä¿®å¤éªŒè¯æµ‹è¯•

#### æµ‹è¯•éªŒè¯
```bash
# è¿è¡ŒGPTQä¿®å¤æµ‹è¯•
python3 test_gptq_fix.py

# è¿è¡Œç®€å•æµ‹è¯•
python3 simple_gptq_test.py
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### 1. å†…å­˜ä¼˜åŒ–
- **æƒé‡ç¼“å­˜**: æ™ºèƒ½ç¼“å­˜å·²åŠ è½½çš„æƒé‡æ–‡ä»¶
- **é€‰æ‹©æ€§åŠ è½½**: åªåŠ è½½éœ€è¦çš„æƒé‡
- **å†…å­˜æ˜ å°„**: æ”¯æŒå¤§æ–‡ä»¶çš„æ‡’åŠ è½½

### 2. è®¡ç®—ä¼˜åŒ–
- **æ··åˆç²¾åº¦**: å¹³è¡¡ç²¾åº¦å’Œæ€§èƒ½
- **å‘é‡åŒ–æ“ä½œ**: ä¼˜åŒ–çš„GPTQåé‡åŒ–
- **å¹¶è¡Œå¤„ç†**: æ”¯æŒå¹¶å‘è¯·æ±‚

### 3. å­˜å‚¨ä¼˜åŒ–
- **å‹ç¼©å­˜å‚¨**: æ”¯æŒå¤šç§å‹ç¼©æ ¼å¼
- **ç´¢å¼•æ–‡ä»¶**: åˆ©ç”¨safetensorsç´¢å¼•
- **åˆ†ç‰‡åŠ è½½**: æ”¯æŒå¤§æ¨¡å‹çš„åˆ†ç‰‡åŠ è½½

## ğŸ”§ è®¾å¤‡é—®é¢˜ä¿®å¤

### 1. è®¾å¤‡ä¸åŒ¹é…é—®é¢˜
è‡ªåŠ¨æ£€æµ‹å’Œä¿®å¤CUDA/CPUè®¾å¤‡ä¸åŒ¹é…é—®é¢˜ï¼š

```python
from fix_device_issues import comprehensive_device_fix

# ç»¼åˆè®¾å¤‡ä¿®å¤
results = comprehensive_device_fix(model, tokenizer, 'cuda')
print(f"ä¿®å¤ç»“æœ: {results}")
```

### 2. æ³¨æ„åŠ›æ©ç ä¿®å¤
è‡ªåŠ¨å¤„ç†pad_tokenå’Œeos_tokenç›¸åŒçš„æƒ…å†µï¼š

```python
from fix_device_issues import create_proper_attention_mask

# åˆ›å»ºæ­£ç¡®çš„æ³¨æ„åŠ›æ©ç 
attention_mask = create_proper_attention_mask(input_ids, tokenizer, 'cuda')
```

### 3. MoEæ¨¡å—è®¾å¤‡ä¿®å¤
ä¸“é—¨å¤„ç†MoEæ¨¡å—çš„è®¾å¤‡é—®é¢˜ï¼š

```python
from fix_device_issues import fix_moe_device_issues

# ä¿®å¤MoEæ¨¡å—è®¾å¤‡é—®é¢˜
model = fix_moe_device_issues(model, 'cuda')
```

### 4. è®¾å¤‡ä¸€è‡´æ€§éªŒè¯
éªŒè¯æ¨¡å‹æ‰€æœ‰å‚æ•°éƒ½åœ¨æ­£ç¡®è®¾å¤‡ä¸Šï¼š

```python
from fix_device_issues import validate_model_device_consistency

# éªŒè¯è®¾å¤‡ä¸€è‡´æ€§
validation = validate_model_device_consistency(model, 'cuda')
if validation['is_consistent']:
    print("âœ“ è®¾å¤‡ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")
else:
    print(f"âš  å‘ç°è®¾å¤‡é—®é¢˜: {validation['issues']}")
```

## ğŸ§ª æµ‹è¯•å’ŒéªŒè¯

### 1. åŠŸèƒ½æµ‹è¯•
```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
python3 test_enhanced_features.py

# æµ‹è¯•ç‰¹å®šåŠŸèƒ½
python3 -c "
from sglang.srt.model_loader.enhanced_mixed_precision_loader import GPTQDequantizer
import torch
# æµ‹è¯•GPTQåé‡åŒ–
qweight = torch.randint(0, 16, (256, 768), dtype=torch.int32)
qzeros = torch.randint(0, 16, (16, 96), dtype=torch.int32)
scales = torch.randn(16, 768, dtype=torch.float16)
weight = GPTQDequantizer.dequantize_gptq_weight(qweight, qzeros, scales)
print(f'GPTQåé‡åŒ–æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {weight.shape}')
"
```

### 2. æ€§èƒ½æµ‹è¯•
```python
import time
from sglang.srt.enhanced_model_loader import load_model_with_enhanced_features

# æµ‹è¯•åŠ è½½æ—¶é—´
start_time = time.time()
stats = load_model_with_enhanced_features(model, config_path)
load_time = time.time() - start_time
print(f"æ¨¡å‹åŠ è½½æ—¶é—´: {load_time:.2f}ç§’")
```

## ğŸ” æ•…éšœæ’é™¤

### 1. å¸¸è§é—®é¢˜

**Q: é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯**
```bash
# æ£€æŸ¥YAMLæ ¼å¼
python3 -c "import yaml; yaml.safe_load(open('config.yaml'))"
```

**Q: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨**
```bash
# æ£€æŸ¥æ¨¡å‹è·¯å¾„
ls -la /path/to/model/
```

**Q: ä¸“å®¶è·Ÿè¸ªä¸å·¥ä½œ**
```python
# æ£€æŸ¥ä¸“å®¶è·Ÿè¸ªå™¨
from sglang.srt.model_loader.enhanced_mixed_precision_loader import get_global_expert_tracker
tracker = get_global_expert_tracker()
if tracker:
    print("ä¸“å®¶è·Ÿè¸ªå™¨å·²å¯ç”¨")
else:
    print("ä¸“å®¶è·Ÿè¸ªå™¨æœªå¯ç”¨")
```

**Q: è®¾å¤‡ä¸åŒ¹é…é”™è¯¯**
```
WARNING: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# è¿è¡Œè®¾å¤‡ä¿®å¤æµ‹è¯•
python3 test_device_fix.py

# ä½¿ç”¨è®¾å¤‡ä¿®å¤åŠŸèƒ½
from fix_device_issues import comprehensive_device_fix
results = comprehensive_device_fix(model, tokenizer, 'cuda')
```

**Q: æ³¨æ„åŠ›æ©ç è­¦å‘Š**
```
The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
```

**è§£å†³æ–¹æ¡ˆ**:
- ç³»ç»Ÿä¼šè‡ªåŠ¨åˆ›å»ºæ­£ç¡®çš„æ³¨æ„åŠ›æ©ç 
- ç¡®ä¿tokenizerçš„pad_token_idå’Œeos_token_idè®¾ç½®æ­£ç¡®

### 2. è°ƒè¯•æ¨¡å¼
```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
export PYTHONPATH=/path/to/sglang/python:$PYTHONPATH
python3 -u launch_enhanced_server.py --config config.yaml --model /path/to/model
```

## ğŸ“š APIå‚è€ƒ

### EnhancedModelLoader
```python
class EnhancedModelLoader:
    def __init__(self, config_path: str, enable_expert_tracking: bool = True)
    def load_model(self, model: torch.nn.Module, enable_moe_tracking: bool = True) -> Dict[str, Any]
    def get_expert_tracker(self) -> Optional[ExpertActivationTracker]
    def get_expert_stats(self, layer_id: Optional[int] = None, expert_id: Optional[int] = None) -> Dict
    def get_top_experts(self, top_k: int = 10) -> list
    def get_layer_stats(self) -> Dict
    def reset_expert_stats(self)
    def export_expert_stats(self, file_path: str)
```

### GPTQDequantizer
```python
class GPTQDequantizer:
    @staticmethod
    def dequantize_gptq_weight(qweight: torch.Tensor, qzeros: torch.Tensor, 
                              scales: torch.Tensor, g_idx: Optional[torch.Tensor] = None,
                              bits: int = 4, group_size: int = 128) -> torch.Tensor
```

### ExpertActivationTracker
```python
class ExpertActivationTracker:
    def record_expert_activation(self, layer_id: int, expert_id: int, 
                               tokens_processed: int = 1, request_id: str = None)
    def record_request(self, request_id: str, input_length: int, output_length: int)
    def get_expert_stats(self, layer_id: Optional[int] = None, expert_id: Optional[int] = None) -> Dict
    def get_top_experts(self, top_k: int = 10) -> List[Dict]
    def get_layer_stats(self) -> Dict[int, Dict]
    def reset_stats(self)
    def export_stats(self, file_path: str)
```

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›è¿™ä¸ªé¡¹ç›®ï¼

### å¼€å‘ç¯å¢ƒè®¾ç½®
```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd sglang-0.4.7

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# è¿è¡Œæµ‹è¯•
python3 test_enhanced_features.py
```

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäºSGLangçš„è®¸å¯è¯ï¼Œè¯·å‚è€ƒåŸå§‹é¡¹ç›®çš„è®¸å¯è¯æ–‡ä»¶ã€‚

## ğŸ™ è‡´è°¢

æ„Ÿè°¢SGLangå›¢é˜Ÿæä¾›çš„é«˜æ€§èƒ½LLMæ¨ç†æ¡†æ¶ï¼Œä»¥åŠå¼€æºç¤¾åŒºå¯¹GPTQå’ŒMoEæŠ€æœ¯çš„è´¡çŒ®ã€‚
